# -*- coding: utf-8 -*-
"""055_ass2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HrbG8Zza-Rg7PQCZfjvQc-R7OKvuKgmX

<div class="alert alert-block alert-success">
    
# **FIT5196 Tasks 1 & 2 - Assessment 2**
#### Student Name: Alexandra Goh & Sothearith Tith
#### Student ID: 29796431 & 27208001

Date: 3rd May 2024


Environment: Python 3.10.12

Libraries used:
* re (for regular expression, installed and imported)
* json (for serialization of Python data structures into JSON format, installed and imported)
    
</div>

<div class="alert alert-block alert-danger">
    
## **Table of Contents**

</div>    

[1. Introduction](#Intro) <br>
[2. Importing Libraries & Reading Data](#read_libs) <br>
$\;\;\;\;$[2.1. Importing Libraries](#import_lib) <br>
$\;\;\;\;$[2.2. Reading Data Files](#read_data) <br>
[Task 1](#task1) <br>
[3. Missing data](#missing_data) <br>
$\;\;\;\;$[3.1. Examining Missing Data](#exam_missing_data) <br>
$\;\;\;\;$[3.2. Preparing Data](#prep_missing_data) <br>
$\;\;\;\;$[3.3. Training Linear Model](#linear_model) <br>
$\;\;\;\;$[3.4. Predict Delivery Fee](#predict_fee) <br>
[4. Dirty Data](#dirty_data) <br>
$\;\;\;\;$[4.1. Examining Dirty Data](#exam_dirty_data) <br>
$\;\;\;\;$[4.2. Examining date](#exam_date) <br>
$\;\;\;\;$[4.3. Examining customer_lat and customer_lon](#exam_lat_lon) <br>
$\;\;\;\;$[4.4. Examining branch_code](#exam_branch_code) <br>
$\;\;\;\;$[4.5. Examining Orders](#exam_orders) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.5.1. Examining order_type](#exam_ordertype) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.5.2. Examining order_items](#exam_orderitems) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.5.3. Examining order_price](#exam_orderprice) <br>
$\;\;\;\;$[4.6. Examining distance_to_customer_KM](#exam_distance) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.6.1. Create Graph](#creat_graph) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.6.2. Assign node to customers & branches](#assign_node) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.6.3. Find shortest distance](#find_distance) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.6.4. Examining distance_to_customer_KM & branch_code](#exam_distnce_branch) <br>
$\;\;\;\;$[4.7. Examining customerHasloyalty](#exam_loyalty) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.7.1. Prepare data for prediction](#prepare_prediction_dirty) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.7.2. Predict delivery_fee](#predict_delivery_fee_dirty) <br>
$\;\;\;\;\;\;\;\;\;\;$[4.7.3. Examining customerHasloyalty?](#exam_loyalty) <br>
[5. Outlier Data](#outlier_data) <br>
$\;\;\;\;$[5.1. Examining Outlier Data](#exam_outlier_data) <br>
$\;\;\;\;$[5.2. Data Transformation & Prediction](#transform_data_prediction) <br>
$\;\;\;\;$[5.3. Detect, Visualise & Remove Outliers](#detect_visualise_remove_outlier) <br>
$\;\;\;\;\;\;\;\;\;\;$[5.3.1. Branch NS](#ns) <br>
$\;\;\;\;\;\;\;\;\;\;$[5.3.2. Branch TP](#tp) <br>
$\;\;\;\;\;\;\;\;\;\;$[5.3.3. Branch BK](#bk) <br>
$\;\;\;\;\;\;\;\;\;\;$[5.3.4. Merge Dataframe](#merge_dataframe) <br>
[6. Data Reshaping](#data_reshaping) <br>
$\;\;\;\;$[6.1. Exploring suburb_info data](#explore_task2_data) <br>
$\;\;\;\;$[6.2. Scaling Methods](#scaling_methods) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.2.1. Z-Score Normalization (Standardization)](#z_score) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.2.2. Log Scaling](#log_scaling) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.2.3. Min-Max Scaling](#min_max) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.2.4. Robust Scaling](#robust) <br>
$\;\;\;\;$[6.3. Transformation Methods (Visualizations)](#transform_methods) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.3.1. number_of_houses & number_of_units](#houses_units) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.3.2. aus_born_perc](#aus_born_perc) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.3.3. population](#population) <br>
$\;\;\;\;$[6.4. Testing with Linear Regression Model](#linear_reg) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.4.1. Linear Regression Analysis on Suburb Data (Before Transformation)](#before_trans) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.4.2. Transformations on number_of_houses](#trans_houses) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.4.3. Transformations on number_of_units](#trans_units) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.4.4. Transformations on aus_born_perc](#trans_aus_born_perc) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.4.5. Transformations on population](#trans_population) <br>
$\;\;\;\;\;\;\;\;\;\;$[6.4.5. Linear Regression Analysis on Suburb Data (After Transformation)](#after_trans) <br>
[7. Writing Solutions to CSV File](#write_csv) <br>
[Task 2](#task2) <br>
$\;\;\;\;$[7.1. Dirty Data Solution](#write_dirty_data) <br>
$\;\;\;\;$[7.2. Outlier Data Solution](#write_outlier_data) <br>
$\;\;\;\;$[7.3. Missing Data Solution](#write_missing_data) <br>
[8. Summary](#summary) <br>
[9. References](#Ref) <br>

-------------------------------------

<div class="alert alert-block alert-warning">

## **1.  Introduction**  <a class="anchor" name="Intro"></a>
    
</div>

This assessment regards the comprehensive exploration, cleansing, and reshaping of datasets pertaining to food delivery services and suburb-related information in Melbourne, Australia. Our primary objective is to ensure the accuracy, integrity, and suitability of the data for subsequent analysis and modeling. Through meticulous data wrangling techniques, we endeavor to uncover hidden patterns, rectify anomalies, and prepare the data for insightful interpretation and predictive modeling.

To achieve this, we are tasked with two key responsibilities:

1.  Data Cleansing:

    Our initial focus lies on detecting and rectifying errors, imputing missing values, and identifying and removing outliers within the food delivery dataset. This dataset comprises orders from a restaurant with three branches in the central business district (CBD) area of Melbourne. Each order instance provides valuable insights into customer preferences, delivery logistics, and operational dynamics across the restaurant's branches.


2.  Data Reshaping:

    Subsequently, we explore the `suburb_info` dataset, aiming to understand the distribution and characteristics of various suburb-related attributes. Our objective is to prepare this data for potential utilization in predictive modeling, particularly focusing on the `median house price` as the target variable. Through the exploration of normalization and transformation techniques, we seek to enhance the data's suitability for linear regression modeling, ensuring that features are appropriately scaled and exhibit linear relationships with the target variable.

-------------------------------------

<div class="alert alert-block alert-warning">
    
## **2.  Importing Libraries & Reading Data** <a class="anchor" name="read_libs"></a>
 </div>

<div class="alert alert-block alert-info">
    
### **2.1. Importing Libraries** <a class="anchor" name="import_lib"></a>

The packages used in this assessment are imported as follows. Each package is utilized to fulfill specific tasks within the analysis:

* **pandas:** for data manipulation and analysis, providing data structures and operations for manipulating numerical tables and time series.
* **numpy:** for numerical computing, offering support for arrays, matrices, and a large collection of mathematical functions.
* **datetime:** for manipulating dates and times.
* **ast:** for parsing and evaluating expressions from Python source code strings.
* **networkx:** for the creation, manipulation, and study of complex networks and graphs.
* **statsmodels.api:** for estimating and testing statistical models.
* **dijkstra_path_length (from networkx.algorithms.shortest_paths.weighted):** for computing the shortest path lengths using Dijkstraâ€™s algorithm.
* **literal_eval (from ast):** for safely evaluating strings containing Python expressions.
* **defaultdict (from collections):** for creating dictionaries with default values for nonexistent keys.
* **scipy:** for scientific and technical computing, providing modules for optimization, integration, interpolation, eigenvalue problems, algebraic equations, and other tasks.
* **boxcox (from scipy.stats):** for applying the Box-Cox transformation to make data more normally distributed.
* **StandardScaler, PolynomialFeatures, MinMaxScaler, QuantileTransformer, RobustScaler (from sklearn.preprocessing):** for scaling and transforming features to improve the performance of machine learning algorithms.
* **train_test_split (from sklearn.model_selection):** for splitting datasets into training and testing sets.
* **LinearRegression, Lasso (from sklearn.linear_model):** for performing linear and Lasso regression analysis.
* **preprocessing (from sklearn):** for various preprocessing utilities.
* **r2_score, mean_squared_error, mean_absolute_error (from sklearn.metrics):** for evaluating the performance of regression models.
* **make_pipeline (from sklearn.pipeline):** for constructing a pipeline to streamline data processing and model training.
* **variance_inflation_factor (from statsmodels.stats.outliers_influence):** for assessing multicollinearity in regression models.
* **cross_val_score (from sklearn.model_selection):** for evaluating model performance using cross-validation.
* **matplotlib:** for creating static, animated, and interactive visualizations.
* **pylab (from matplotlib):** a module that gets installed alongside matplotlib and provides a Matlab-like interface.
* **seaborn:** for making statistical graphics in Python, built on top of matplotlib.
* **qqplot (from statsmodels.graphics.gofplots):** for creating Q-Q plots to assess if a dataset follows a given distribution.
"""

#Basic scientific python libs
import pandas as pd
import numpy as np
import datetime
import ast
import networkx as nx
import statsmodels.api as sm
from networkx.algorithms.shortest_paths.weighted import dijkstra_path_length
from ast import literal_eval
from collections import defaultdict
from scipy import stats
from scipy.stats import boxcox
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, QuantileTransformer, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso
from sklearn import preprocessing
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import make_pipeline
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import cross_val_score

# Visualisation
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import seaborn as sns
from statsmodels.graphics.gofplots import qqplot

!pip install pandas-profiling[notebook]

"""<div class="alert alert-block alert-info">
    
### **2.2. Reading Data Files** <a class="anchor" name="read_data"></a>

For Task 1, we load the following datasets:

*   **dirty_data:** Contains the raw data with potential errors.
*   **outlier_data:** Consists of data with potential outlier values, specifically focusing on the `delivery_fee` attribute.
*   **missing_data:** Contains data with missing values, specifically focusing on the `delivery_fee` attribute.
*   **nodes:** Supplementary data providing information about nodes.
*   **edges:** Supplementary data containing information about edges.
*   **branches:** Supplementary data providing details about the branches of the restaurant.

For Task 2, we utilize:

*   **suburb_info:** This dataset, extracted from an Excel file, contains suburb-related information necessary for our data reshaping task.

By loading these datasets, we lay the foundation for our subsequent data exploration, cleansing, and reshaping tasks, enabling us to extract meaningful insights and derive actionable conclusions.
"""

from google.colab import drive
drive.mount('/content/drive')

# for task 1

dirty_data = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2024/A2/student_data/Group055_dirty_data.csv")
outlier_data = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2024/A2/student_data/Group055_outlier_data.csv")
missing_data = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2024/A2/student_data/Group055_missing_data.csv")
nodes = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2024/A2/supplementary_data/nodes.csv")
edges = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2024/A2/supplementary_data/edges.csv")
branches = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2024/A2/supplementary_data/branches.csv")

# for task 2

suburb_info = pd.read_excel("/content/drive/Shareddrives/FIT5196_S1_2024/A2/supplementary_data/suburb_info.xlsx")

"""-------------------------------------

<div class="alert alert-block alert-warning">

## <u>**Task 1**<u>  <a class="anchor" name="task1"></a>
    
</div>

<div class="alert alert-block alert-warning">

## **3.  Missing Data** <a class="anchor" name="missing_data"></a>

</div>

**Steps to Build a Linear Regression Model for Predicting Missing Values**

1. Data Preparation

* Identify Missing Values: Identify which records have missing values in the target variable.
* Subset Data: Create subsets of the data where the target variable is not missing (to train the model) and where it is missing (to predict).
* Feature Selection: Choose the features that will be used to predict the missing values.

2. Split Data

* Train-Test Split: Split the subset of data with known target values into a training set and a test set to evaluate model performance.

3. Model Training

* Instantiate Model: Create an instance of the Linear Regression model.
* Fit Model: Train the model using the training data.

4. Model Evaluation

* Predict and Evaluate: Use the test set to predict the target variable and evaluate the model performance using metrics like R-squared, Mean Absolute Error (MAE), or Root Mean Squared Error (RMSE).

5. Predict Missing Values

* Prepare Features: Ensure that the feature set for the rows with missing target values is ready.
* Predict Missing Values: Use the trained model to predict the missing values.

6. Impute Missing Values

* Update DataFrame: Replace the missing values with the predicted values.

<div class="alert alert-block alert-info">
    
### **3.1. Examining Missing Data** <a class="anchor" name="exam_missing_data"></a>

To impute the missing values, we can begin by analyzing the relationship between the available values and the missing values that need to be imputed. In our dataset, there are three columns with missing values: `branch_code`, `distance_to_customer_KM`, and `delivery_fee`. However, we will focus only on imputing the missing values in the `delivery_fee` column.

We know that `delivery_fee` is linearly dependent on `date`, `time`, and `distance_to_customer_KM`. Since `date` and `time` are continuous variables, we need to convert them into discrete categories for linear models. This categorization will ensure that each category has a sufficient number of observations, leading to more stable and reliable estimates and making the model easier to interpret.

The `order_type` variable is already categorized, but since a linear model cannot process strings, we need to convert 'Breakfast', 'Lunch', and 'Dinner' into numerical representations. Specifically, we will map 'Breakfast' to '1', 'Lunch' to '2', and 'Dinner' to '3'.
"""

print (missing_data.shape)
missing_data.info()

"""<div class="alert alert-block alert-info">
    
### **3.2. Preparing Data** <a class="anchor" name="prep_missing_data"></a>

Our first step is to prepare the data for training the model. We will include only the rows that have `delivery_fee` values, as the model needs to find the relationship between `delivery_fee` and other attributes. We have 450 rows available for the training set. However, we noticed some missing values in the `branch_code` and `distance_to_customer_KM` columns, with 100 and 50 values missing, respectively.

We could impute these missing values using various methods, such as assigning the most frequent value to `branch_code` and calculating the mean for `distance_to_customer_KM`. However, since these imputations are based on estimations, there is a risk of introducing bias into the model. Thus, we decided to remove them completely.
"""

train_df = missing_data[missing_data['delivery_fee'].notna()].copy()
train_df.info()

train_df = train_df.dropna(subset=['distance_to_customer_KM'])

train_df = train_df.dropna(subset=['branch_code'])

"""After removing rows with missing values, we are left with 350 rows in the training set."""

train_df.info()

"""Now, we have to prepare a dataset for prediction where we will use the trained model to impute the missing `delivery_fee`. We see that the dataset has no missing values in any row beside `delivery_fee`, thus, we do not need to do any filtering."""

predict_df = missing_data[missing_data['delivery_fee'].isna()].copy()
predict_df.info()

"""Here, we defined a function to categorize dates into weekdays (0) and weekends (1). This function was then applied to the `date` column in both the `train_df` and `predict_df` dataframes.

The `date.dayofweek` attribute returns an integer representing the day of the week (i.e. 0 for Monday, 1 for Tuesday, ..., 6 for Sunday).
"""

# Define a function to categorize day into weekday and weekend
def change_date(date):
  # Ensure the 'date' column is in datetime format
  date = pd.to_datetime(date)
  if date.dayofweek > 4:  # weekend
    return 1
  return 0  # weekday

# Apply transformation to categorize dates as 0 for weekdays and 1 for weekends
train_df['date'] = train_df['date'].apply(change_date)
predict_df['date'] = predict_df['date'].apply(change_date)
train_df.head()

"""In this section, we defined a function to categorize times into `morning (0)`, `afternoon (1)`, and `evening (2)`. This function is then applied to the `time` column in both the `train_df` and `predict_df` dataframes."""

# Define a function to categorize time into breakfast, lunch, and dinner
def categorize_time(time):
    # Ensure the 'time' column is recognized as datetime type
    time = pd.to_datetime(time, format='%H:%M:%S').time()
    if datetime.time(8, 0) <= time <= datetime.time(12, 0):
        return 0  # Breakfast
    elif datetime.time(12, 0, 1) <= time <= datetime.time(16, 0):
        return 1  # Lunch
    elif datetime.time(16, 0, 1) <= time <= datetime.time(20, 0):
        return 2  # Dinner
    else:
        return -1  # Outside of defined meal times

# Apply the categorization function to the 'time' column
train_df['time'] = train_df['time'].apply(categorize_time)
predict_df['time'] = predict_df['time'].apply(categorize_time)
train_df.head()

"""Since the `delivery_fee` is influenced by customer loyalty and is linearly dependent on `date`, `time`, and `distance_to_customer_KM`, it is essential to adjust the `delivery_fee` to its original value for the model to make more accurate predictions."""

# Define a function to retsore delivery_fee to original delivery_fee
def update_fee(row):
    if row['customerHasloyalty?'] == 1:
        return row['delivery_fee'] * 2
    return row['delivery_fee']


# Apply the categorization function to the 'branch_code' column
train_df['original_fee'] = train_df.apply(update_fee, axis=1)
# predict_df['original_fee'] = predict_df.apply(update_fee, axis=1)
train_df.head()

"""We know that the delivery fee depends linearly on `date`, `time`, `distance_to_customer_KM`, and other attributes. However, each branch exhibits a different linear dependency. To account for these variations, we split the `training` and `prediction` datasets based on the `branch_code`."""

# Separate the DataFrame based on branch_code values
df_train_1 = train_df[train_df['branch_code'] == 'NS']
df_train_2 = train_df[train_df['branch_code'] == 'TP']
df_train_3 = train_df[train_df['branch_code'] == 'BK']

# Separate the DataFrame based on branch_code values
df_predict_1 = predict_df[predict_df['branch_code'] == 'NS'].copy()
df_predict_2 = predict_df[predict_df['branch_code'] == 'TP'].copy()
df_predict_3 = predict_df[predict_df['branch_code'] == 'BK'].copy()

"""<div class="alert alert-block alert-info">
    
### **3.3. Training Linear Model** <a class="anchor" name="linear_model"></a>

Now, we prepare the features and target variables for training a linear model to predict the `delivery_fee` for different branches. The dataset is divided into three separate subsets based on `branch_code`. For each branch, we then perform feature selection, extract the target variable, and split the data into training and test sets.

For each branch, we select features for the linear model by excluding certain columns from the DataFrame (`df_train_1`, `df_train_2`, `df_train_3`). The columns excluded are `order_id`, `order_items`, `delivery_fee`, `customer_lat`, `customer_lon`, and `branch_code`.

The target variable (i.e. `y_1`, `y_2`, `y_3`) is extracted as the `delivery_fee` column from each respective DataFrame.

The data for each branch is then split into training and test sets using the `train_test_split` function from the `sklearn.model_selection` module. The parameter `train_size=0.7` indicates that 70% of the data will be used for training, while the remaining 30% will be used for testing. The `random_state=3` parameter ensures reproducibility of the splits by setting a seed for the random number generator.
"""

# Prepare features and target for the known dataset
features_branch_1 = [x for x in df_train_1.columns if x not in ['order_id', 'order_items', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'delivery_fee', 'original_fee']]
features_branch_2 = [x for x in df_train_2.columns if x not in ['order_id', 'order_items', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'delivery_fee', 'original_fee']]
features_branch_3 = [x for x in df_train_3.columns if x not in ['order_id', 'order_items', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'delivery_fee', 'original_fee']]

X_1 = df_train_1[features_branch_1]
X_2 = df_train_2[features_branch_2]
X_3 = df_train_3[features_branch_3]

y_1 = df_train_1['original_fee']
y_2 = df_train_2['original_fee']
y_3 = df_train_3['original_fee']

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, train_size=0.7, random_state = 111)
X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, train_size=0.7, random_state = 1)
X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, train_size=0.7, random_state = 12)

"""We instantiate three separate linear regression models, `linearReg_1`, `linearReg_2`, and `linearReg_3` for each branch. These models will later be trained on the respective training dataset for each branch, allowing for separate linear models to be fitted for each branch's unique data characteristics."""

linearReg_1 = LinearRegression() #instatiate
linearReg_2 = LinearRegression() #instatiate
linearReg_3 = LinearRegression() #instatiate

"""Next, we train all three models by calling the `fit` method on each model with the appropriate features and target variable. This enables the models to learn the relationships between the input features and the delivery fee for each branch."""

linearReg_1.fit(X_train_1,y_train_1) #fit
linearReg_2.fit(X_train_2,y_train_2) #fit
linearReg_3.fit(X_train_3,y_train_3) #fit

"""Finally, we evaluate the performance of the three trained linear regression models by calculating their R-squared values on the respective test datasets (`X_test_1`, `y_test_1`, `X_test_2`, `y_test_2`, and `X_test_3`, `y_test_3`). The R-squared value measures how well the model's predictions match the actual data, indicating the proportion of the variance in the delivery fee that is explained by the features.

According to the output below:
* For branch NS, the R-squared value is 0.977, suggesting that approximately 97.7% of the variance in the delivery fee is explained by the features in the model. This indicates a strong fit between the predicted and actual delivery fees for this branch.
* For branch TP, the R-squared value is 0.967, indicating that around 96.7% of the variance in the delivery fee is explained by the features. Similar to branch NS, this suggests a strong fit between the predicted and actual delivery fees for branch TP.
* For branch BK, the R-squared value is 0.990, indicating that approximately 99.0% of the variance in the delivery fee is explained by the features. This represents an exceptionally high level of explanatory power and suggests an excellent fit between the predicted and actual delivery fees for branch BK.

Overall, these high R-squared values indicate that the linear regression models perform well in capturing the relationships between the input features and the delivery fee for each branch, demonstrating strong predictive capability.
"""

print ('r-squared for this model(branch NS) = ',linearReg_1.score(X_test_1,y_test_1))
print ('r-squared for this model(branch TP) = ',linearReg_2.score(X_test_2,y_test_2))
print ('r-squared for this model(branch BK) = ',linearReg_3.score(X_test_3,y_test_3))

"""<div class="alert alert-block alert-info">
    
### **3.4. Predict Delivery Fee** <a class="anchor" name="predict_fee"></a>

After training three LinearRegression models for each branch, we then use the trained models to predict `delivery_fee` for new, unseen data (held in `df_predict_1`, `df_predict_2`, and `df_predict_3`).

Here are the steps:

1. Extract relevant features (`features_branch_1`, `features_branch_2`, `features_branch_3`) from the prediction DataFrames (`df_predict_1`, `df_predict_2`, `df_predict_3`).
  
2. Utilize the respective models (`linearReg_1`, `linearReg_2`, `linearReg_3`) to predict `delivery_fee` for these new datasets.

3. Add the predicted `delivery_fee` as a new column, `predicted_delivery_fee`, in each of the prediction DataFrames.
"""

X_unknown1 = df_predict_1[features_branch_1]
df_predict_1['predicted_delivery_fee'] = linearReg_1.predict(X_unknown1)

X_unknown2 = df_predict_2[features_branch_2]
df_predict_2['predicted_delivery_fee'] = linearReg_2.predict(X_unknown2)

X_unknown3 = df_predict_3[features_branch_3]
df_predict_3['predicted_delivery_fee'] = linearReg_3.predict(X_unknown3)

"""To consolidate the data, we combine the three DataFrames (`df_predict_1`, `df_predict_2`, and `df_predict_3`) into a single DataFrame (`combined_df`)."""

# Join the DataFrames
combined_df = pd.concat([df_predict_1, df_predict_2, df_predict_3], ignore_index=True)
combined_df.head()

# Define a function to retsore delivery_fee to original delivery_fee
def restore_fee(row):
    if row['customerHasloyalty?'] == 1:
        return row['predicted_delivery_fee'] / 2
    return row['predicted_delivery_fee']


# Apply the categorization function to the 'branch_code' column
combined_df['original_fee'] = combined_df.apply(restore_fee, axis=1)
combined_df.head()

"""Subsequently, we fill the missing `delivery_fee` values in the `missing_data` dataframe using the `predicted_delivery_fee` from the `combined_df` dataframe. This is done by merging the two dataframes based on `order_id` and then updating the missing `delivery_fee` values with `predicted_delivery_fee`."""

# Fill the missing delivery_fee in the original DataFrame with the predictions
merged_df = pd.merge(missing_data, combined_df[['order_id', 'predicted_delivery_fee']], on='order_id', how='left')
# merged_df.head()

# Fill the missing 'delivery_fee' in missing_data with the 'delivery_fee_combined' from combined_df
missing_data['delivery_fee'] = missing_data['delivery_fee'].combine_first(merged_df['predicted_delivery_fee'])
missing_data.info()

"""-------------------------------------

<div class="alert alert-block alert-warning">

## **4.  Dirty Data** <a class="anchor" name="dirty_data"></a>

 </div>

<div class="alert alert-block alert-info">
    
### **4.1. Examining Dirty Data** <a class="anchor" name="exam_dirty_data"></a>

Firstly, we examine the structure and initial content (i.e., the first 10 rows) of the `dirty_data` dataset. The dataset consists of 500 rows and 12 columns. This examination allows us to understand the data's structure and identify any potential issues or inconsistencies in the data that will need to be addressed in subsequent cleaning steps.
"""

print (dirty_data.shape)
dirty_data.head(10)

"""The summary below shows that there are no missing values in any of the columns, as each column has 500 non-null entries. The data types are as follows: six columns are of type object (likely containing strings), one column (`customerHasloyalty`) is of type int64 (integer), and five columns are of type float64 (floating-point numbers). This information is crucial for understanding the composition of the dataset and ensuring that all data fields are complete before proceeding with further analysis or preprocessing."""

dirty_data.info()

"""<div class="alert alert-block alert-info">
    
### **4.2. Examining `date`** <a class="anchor" name="exam_date"></a>

Firstly, we identify records that have an incorrect date format. We use a regular expression to match dates in the correct format ('YYYY-MM-DD'). The expression used returns a boolean mask with 'True' indicating rows where the date does not match the specified format.

We then filter the rows with the incorrect date format by applying this mask to the dataset, creating a subset that contains only the rows with incorrectly formatted dates. Upon inspection, we find that there are 18 rows where the dates are in the format 'DD-MM-YYYY'.
"""

# Records with "DD-MM-YYYY" error

# Check which rows do not match the specified format
rows_with_wrong_format = dirty_data[~dirty_data['date'].str.match(r'\d{4}-\d{2}-\d{2}')]

# Display rows with incorrect format
print("Rows with incorrect date format:")
print(rows_with_wrong_format.loc[:, ['date']].to_string())

"""To correct the date format for rows that were previously identified as having an incorrect format ('DD-MM-YYYY'), we use the `pd.to_datetime` function to convert the `date` column to the correct datetime data type for rows with the incorrect format. The `format='%d-%m-%Y'` parameter specifies the original incorrect format so that it can be properly converted.

We then apply this conversion to the specific rows by selecting the indices of those rows and updating the `date` column in the `dirty_data` dataframe. The corrected rows are displayed below:
"""

# Convert 'date' column to datetime data type with the correct format for rows with incorrect format
dirty_data.loc[rows_with_wrong_format.index, 'date'] = pd.to_datetime(rows_with_wrong_format['date'], format='%d-%m-%Y')

# Print the rows with the corrected date format (previously "DD-MM-YYYY")
print("Rows with corrected date format:")
print(dirty_data.loc[rows_with_wrong_format.index, ['date']])

"""Next, we identify and handle records where the date format might have the months and days swapped (i.e., 'YYYY-DD-MM').

Firstly, we create a function `check_for_swapped_dates` that attempts to convert a date string to a datetime object. If the conversion fails (raising a ValueError), the function returns 'True', indicating that the date format might be incorrect.

We then use the `.apply` method to apply our function to each entry in the `date` column. The result is a new column, `swapped_dates`, which contains 'True' for dates that are potentially swapped and 'False' for valid dates.

Subsequently, we filter the dataframe to create `swapped_dates_df`, which includes only the rows where the `swapped_dates` column is 'True'. The dates with months and days swapped are shown as below:
"""

# Records with months and days swapped error ("YYYY-DD-MM")

# Function to check for swapped MM and DD
def check_for_swapped_dates(date_str):
    try:
        pd.to_datetime(date_str)
        return False
    except ValueError:
        return True

# Apply the function to the 'date' column and create a new column indicating if the date is valid or not
dirty_data['swapped_dates'] = dirty_data['date'].apply(check_for_swapped_dates)

# Filter rows where dates are swapped
swapped_dates_df = dirty_data[dirty_data['swapped_dates']]

print(swapped_dates_df[['date']])

"""To address this issue, we define a function `correct_swapped_dates` that attempts to convert a date string to a datetime object. If the conversion fails, indicating a potential swapped month and day, the function swaps the month and day components of the date string and returns the corrected date.

We then apply the `correct_swapped_dates` function selectively to the rows where the `swapped_dates` column is 'True'. This ensures that only dates identified as potentially swapped are corrected, while leaving valid dates unchanged.  Using `.loc`, we specifically target and update the `date` column in the `dirty_data` dataframe for rows with swapped dates.

The corrected dates are displayed below:
"""

# Function to correct dates with swapped month and day
def correct_swapped_dates(date_str):
    try:
        # Try converting to datetime
        pd.to_datetime(date_str)
        return date_str  # If successful, return the original date
    except ValueError:
        # If conversion fails, swap month and day
        year, month, day = date_str.split('-')
        corrected_date = '-'.join([year, day, month])
        return corrected_date

# Apply the function to correct the 'date' column only for rows with swapped dates in dirty_data
dirty_data.loc[dirty_data['swapped_dates'], 'date'] = dirty_data.loc[dirty_data['swapped_dates'], 'date'].apply(correct_swapped_dates)

# Print the corrected DataFrame for only the rows with swapped dates
print("Corrected DataFrame:")
print(dirty_data[dirty_data['swapped_dates']][['date']])

"""This is followed by converting the `date` column in the `dirty_data` dataframe to datetime type. By doing so, we ensure that the dates are represented in a standardized and computationally friendly format."""

# Convert 'date' column to datetime type
dirty_data['date'] = pd.to_datetime(dirty_data['date'], format='%Y-%m-%d')

"""To check the date range, we use the `min()` and `max()` functions on the `date` column to find the earliest (minimum) and latest (maximum) dates in the dataset.

This is to ensure that the temporal scope of the dataset covers a reasonable time period and that the dates fall within expected boundaries. From the output, the data aligns with our expectations and there does not seem to be any extreme outliers or anomalies in the date range.
"""

# Checking for Date Range

min_date = dirty_data['date'].min()
max_date = dirty_data['date'].max()

print(min_date)
print(max_date)

"""To address and flag records with specific errors, a new column `checks` is added to the dataframe with all values initialized to 0. Subsequently, we set the `checks` column to `date1` for rows where the date format is incorrect, as identified earlier. Similarly, for rows with swapped month and day formats, the `checks` column is set to `date2`.

Following the flagging process, the `swapped_dates` column, which was previously used for identification, is removed to streamline the dataframe. This organized approach allows us to mark and track records requiring attention while maintaining data integrity.
"""

# Add a new column 'checks' with all values initialized to 0
dirty_data['checks'] = 0

# Set 'checks' to 1 for rows with incorrect date format (18 wrong records)
dirty_data.loc[rows_with_wrong_format.index, 'checks'] = "date1"

# Set 'checks' to 1 for rows with swapped month and day (19 wrong records)
dirty_data.loc[swapped_dates_df.index, 'checks'] = "date2"

# Delete the "swapped_dates" column
del dirty_data['swapped_dates']

"""<div class="alert alert-block alert-info">
    
### **4.3. Examining `customer_lat` and `customer_lon`** <a class="anchor" name="exam_lat_lon"></a>

To identify the incorrect `lat` and `lon` columns, we merge the `dirty_data` and `nodes` dataframes using a left join on the specified columns. The left join ensures that all rows from `dirty_data` are retained, and rows from `nodes` are included where there is a match on the latitude and longitude columns.

After merging, we identify the rows in `merged_nodes_dirty` where there was no matching entry in `nodes`, as each customer's latitude and longitude combination originally comes from the `nodes` dataset. This is done by checking for null values in the `lat` and `lon` columns, which were added from the `nodes` dataframe.

By identifying these null values, we can pinpoint which geographic coordinates from `dirty_data` did not find a corresponding entry in `nodes`. This helps us identify and investigate any discrepancies or errors in the geographic coordinates in the `dirty_data` dataframe. The unmatched rows and their coordinates are displayed below.
"""

# Merge the dataframes
merged_nodes_dirty = pd.merge(dirty_data, nodes, left_on=["customer_lat", "customer_lon"], right_on=["lat", "lon"], how="left")

# Identify the rows where no match was found
unmatched_rows = merged_nodes_dirty[merged_nodes_dirty["lat"].isnull() | merged_nodes_dirty["lon"].isnull()]

# Print out the row ids, customer_lat, and customer_lon that are not matching
print(unmatched_rows[["customer_lat", "customer_lon"]])

"""To determine the valid range of latitude and longitude values based on the data in the `nodes` dataframe, we calculate the minimum and maximum values for the `lat` and `lon` columns.

From this, we can see that the acceptable range for geographic coordinates is from -38.1109156 to -37.7396359 for latitude and from 144.6541725 to 145.04645 for longitude.
"""

# Get the minimum and maximum values for latitude and longitude from the nodes dataframe
valid_lat_range = (nodes['lat'].min(), nodes['lat'].max())
valid_lon_range = (nodes['lon'].min(), nodes['lon'].max())

# Print the valid ranges
print("Valid latitude range:", valid_lat_range)
print("Valid longitude range:", valid_lon_range)

# Create 'customer_lat_original' and 'customer_lon_original' columns
dirty_data['customer_lat_original'] = dirty_data['customer_lat']
dirty_data['customer_lon_original'] = dirty_data['customer_lon']

"""We start by identifying rows in the `merged_nodes_dirty` dataframe where there was no matching entry in the `nodes` dataframe. These rows have null values in the `lat` and `lon` columns, indicating potential data issues.

For each unmatched row, we check if the `customer_lat` and `customer_lon` values might have been swapped. This is done by checking if a record exists in the `nodes` dataframe where `lat` matches `customer_lon` and `lon` matches `customer_lat`.
If such a record is found, we correct the swapped coordinates by swapping the `customer_lat` and `customer_lon` values in the unmatched_rows dataframe.

After correcting the swapped coordinates in the `unmatched_rows` dataframe, we update the original `dirty_data` dataframe with these corrected values.

"""

# Records with `customer_lat` and `customer_lon` swapped wrongly

# Iterate over the unmatched rows
for index, row in unmatched_rows.iterrows():
    # Check if the lat and lon are swapped
    if nodes[(nodes["lat"] == row["customer_lon"]) & (nodes["lon"] == row["customer_lat"])].shape[0] > 0:
        # Swap lat and lon
        unmatched_rows.at[index, "customer_lat"], unmatched_rows.at[index, "customer_lon"] = row["customer_lon"], row["customer_lat"]

# Update the dirty_data dataframe with the corrected values
for index, row in unmatched_rows.iterrows():
    dirty_data.at[index, "customer_lat"] = row["customer_lat"]
    dirty_data.at[index, "customer_lon"] = row["customer_lon"]

# Set 'checks' to 1 for corrected rows (4 wrong records)
dirty_data.loc[unmatched_rows.index, 'checks'] = "lat1"

"""We also aimed to identify and correct records in the `dirty_data` dataframe where the `customer_lat` values were mistakenly positive instead of negative, as seen earlier when we printed out the wrong records.

Firstly, we define valid ranges and correct signs for the geographical coordinates. This involves checking if `customer_lat` values fall within the valid latitude range and are negative, as latitude values should be within -38.1109156 to -37.7396359. Similarly, we do the same checks for `customer_lon` values, which should fall within the valid positive range (i.e. 144.6541725 to 145.04645).

We then filtered out the invalid rows where either the `customer_lat` or `customer_lon` values did not meet the valid range and correct sign criteria. For these rows, we corrected the `customer_lat` values by changing their sign from positive to negative. This step addresses the error where latitude values were mistakenly positive.
"""

# Records with `customer_lat` that are positive sign instead of negative sign (not within range)

# Check if "customer_lat" and "customer_lon" are within the valid range and have correct signs
valid_lat = (dirty_data['customer_lat'].between(*valid_lat_range)) & (dirty_data['customer_lat'] < 0)
valid_lon = (dirty_data['customer_lon'].between(*valid_lon_range)) & (dirty_data['customer_lon'] > 0)

# Filter out the rows with invalid "customer_lat" or "customer_lon"
invalid_rows = dirty_data[~(valid_lat & valid_lon)]

# Correct latitude values that are positive sign to negative sign
for index, row in invalid_rows.iterrows():
    dirty_data.at[index, 'customer_lat'] = -row['customer_lat']

# Set 'checks' to 1 for corrected rows (37 wrong records)
dirty_data.loc[invalid_rows.index, 'checks'] = "lat2"

"""Finally, we verify that all previously identified and corrected `customer_lat` and `customer_lon` values in the `dirty_data` dataframe now correctly match entries in the `nodes` dataframe by performing a left join merge between the corrected `dirty_data` dataframe and the `nodes` dataframe on the `customer_lat` and `customer_lon` columns.

This allows us to check if the corrected coordinates now match the entries in the `nodes` dataframe.

The empty DataFrame output indicates all previously identified errors in the `customer_lat` and `customer_lon` values have been addressed, and the coordinates now correctly match the entries in the `nodes` dataframe.
"""

# Merge the corrected dirty_data dataframe with nodes to check for matches
corrected_merged_nodes_dirty = pd.merge(dirty_data, nodes, left_on=["customer_lat", "customer_lon"], right_on=["lat", "lon"], how="left")

# Identify the rows where no match was found after correction
corrected_unmatched_rows = corrected_merged_nodes_dirty[corrected_merged_nodes_dirty["lat"].isnull() | corrected_merged_nodes_dirty["lon"].isnull()]

# Print out the row ids, customer_lat, and customer_lon that are still not matching after correction
print("Unmatched rows after correction:")
print(corrected_unmatched_rows[["customer_lat", "customer_lon"]])

"""To compare the original and corrected coordinates, we printed the `order_id`, `customer_lat_original` (before correction), `customer_lat` (after correction), `customer_lon_original` (before correction), and `customer_lon` (after correction) columns to compare the original and corrected coordinates."""

# Create a boolean mask where each value indicates whether the original and corrected 'lat' and 'lon' values differ
mask = (dirty_data['customer_lat_original'] != dirty_data['customer_lat']) | (dirty_data['customer_lon_original'] != dirty_data['customer_lon'])

# Use the mask to create a new dataframe that only contains the rows where the original and corrected 'lat' and 'lon' values differ
corrected_rows = dirty_data[mask]

# Print the corrected rows
corrected_rows[['order_id', 'customer_lat_original', 'customer_lat', 'customer_lon_original', 'customer_lon']]

# Drop 'customer_lat_original' and 'customer_lon_original' columns
dirty_data = dirty_data.drop(columns=['customer_lat_original', 'customer_lon_original'])

"""<div class="alert alert-block alert-info">
    
### **4.4. Examining `branch_code`** <a class="anchor" name="exam_branch_code"></a>

Next, we aim to identify branch codes that are in lower-case instead of upper-case.

We used the `unique()` function to extract all unique values from the `branch_code` column in the `dirty_data` dataframe. This provides a list of distinct branch codes present in the data.

It is revealed that there are some entries with branch codes in lower-case ('bk', 'ns', 'tp') instead of the expected upper-case format ('TP', 'NS', 'BK'). This indicates a data quality issue where branch codes are inconsistently formatted.
"""

# some branch_codes lower-case instead of upper-case

unique_values = dirty_data['branch_code'].unique()
print(unique_values)

"""A new dataframe, `lower_case_branch_codes_before`, which contains all rows from `dirty_data` where the `branch_code` is in lower-case, is created. This helps us isolate the records that need correction.

We then applied the `upper()` function to convert all branch codes in the `dirty_data` dataframe to upper-case. This standardizes the branch codes and resolves the inconsistency.
"""

# Identify rows with lower-case branch codes before correction
lower_case_branch_codes_before = dirty_data[dirty_data['branch_code'].str.islower()]

dirty_data['branch_code'] = dirty_data['branch_code'].apply(lambda x: x.upper())

# Set 'checks' to 1 for corrected rows (29 wrong records)
dirty_data.loc[lower_case_branch_codes_before.index, 'checks'] = "branch1"

"""By re-extracting and examining the unique branch codes, we validated that all branch codes are now consistently formatted in upper-case."""

unique_values = dirty_data['branch_code'].unique()
print(unique_values)

"""<div class="alert alert-block alert-info">
    
### **4.5. Examining Orders** <a class="anchor" name="exam_orders"></a>

<div class="alert alert-block alert-info">
    
#### **4.5.1. Examining `order_type`** <a class="anchor" name="exam_ordertype"></a>

In terms of order type, we can see there are three kinds, namely 'Breakfast', 'Lunch' and 'Dinner'.
"""

print(dirty_data['order_type'].unique())

"""The bar graph below illustrates the distribution of various order types across different hours of the day, providing insights into the ordering patterns throughout the day.

Upon observation, it is evident that certain orders are misclassified. For instance, some 'Lunch' orders are recorded during early morning hours (around 8-11 AM) or in the late afternoon (around 4-5 PM). Additionally, several 'Breakfast' and 'Dinner' orders are incorrectly logged between 12 PM and 4 PM, which deviates from the typical serving times.

According to the correct timing standards:
*   Breakfast orders should be served during the morning hours (8:00 AM to 12:00 PM).
*   Lunch orders are expected during the afternoon (12:00:01 PM to 4:00 PM).
*   Dinner orders are traditionally served in the evening (4:00:01 PM to 8:00 PM).

Therefore, rectifying these misclassifications is essential to ensure that orders are accurately categorized according to their respective meal times.
"""

# Extract the hour from the 'time' column
dirty_data['hour'] = dirty_data['time'].str.slice(0, 2)

# Group the data by hour and 'order_type', and count the occurrences
grouped_data = dirty_data.groupby(['hour', 'order_type']).size().unstack(fill_value=0)

# Plot the bar graph
plt.figure(figsize=(12, 8))
grouped_data.plot(kind='bar', stacked=True)
plt.xlabel('Hour')
plt.ylabel('Count')
plt.title('Order Types for Different Hours')
plt.xticks(rotation=45)
plt.legend(title='Order Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(axis='y')
plt.show()

"""Firstly, the expected timings for each meal ('Breakfast', 'Lunch', and 'Dinner') are defined as tuples containing the start and end hours for each meal period.

We then define a `check_timing` function, which takes an hour and a meal as input and returns `True` if the hour falls within the expected timing for the specified meal, and `False` otherwise.

Subsequently, the hour component is extracted from the `time` column in the `dirty_data` DataFrame using string slicing, converted to an integer, and stored in a new column named `hour`. For each record in the `dirty_data` DataFrame, the hour value is checked against the expected timing for the corresponding order type (meal).

If the hour does not fall within the expected timing for the labeled meal, the record's index, order type, time, and `checks` value are appended to a list named `incorrectly_labeled`. A DataFrame named `incorrect_df` is then created from the list of incorrectly labeled records.
"""

# Define the expected timings for each meal
expected_timings = {
    'Breakfast': (8, 12),  # Breakfast: 08:00:00 to 12:00:00
    'Lunch': (12, 16),      # Lunch: 12:00:00 to 16:00:00
    'Dinner': (16, 21)      # Dinner: 16:00:00 to 21:00:00
}

# Function to check if the hour falls within the expected timing for a given meal
def check_timing(hour, meal):
    start_hour, end_hour = expected_timings[meal]
    return start_hour <= hour < end_hour

# Extract the hour from the 'time' column
dirty_data['hour'] = dirty_data['time'].str.slice(0, 2).astype(int)

# Check if each record's hour falls within the expected timing for its labeled meal
incorrectly_labeled = []
for index, row in dirty_data.iterrows():
    if not check_timing(row['hour'], row['order_type']):
        incorrectly_labeled.append({'Index': index, 'Order Type': row['order_type'], 'Time': row['time'], 'Checks': row['checks']})
        dirty_data.at[index, 'checks'] = "ordertype1"

# Create a DataFrame from the list of incorrectly labeled records (37 wrong records)
incorrect_df = pd.DataFrame(incorrectly_labeled)

incorrect_df

"""The `categorize_order` function takes an hour as input and categorizes the order type ('Breakfast', 'Lunch', or 'Dinner') based on the hour. The categorization follows the expected meal timings previously defined.

The code iterates over each incorrectly labeled record stored in the DataFrame `incorrect_df`. For each misclassified record, it extracts the hour from the `Time` column and converts it to an integer.

The `categorize_order` function is then called with the extracted hour to determine the correct order type based on the hour, since the `time` column in the original `dirty_data` DataFrame is free from errors. The correct order type determined by the `categorize_order` function is later assigned to the corresponding record in the `dirty_data` DataFrame. To locate the correct record in `dirty_data`, the `Index` column from `incorrect_df` is used, and the `order_type` column is updated with the correct value.

By applying the `categorize_order` function to each incorrectly labeled record, this ensures that the order types are corrected based on the actual hour of the order and helps align the order classifications with the expected meal timings, addressing the misclassifications identified earlier.
"""

# Define a function to categorize the order_type based on the hour
def categorize_order(hour):
    if 8 <= hour < 12:
        return 'Breakfast'
    elif 12 <= hour < 16:
        return 'Lunch'
    elif 16 <= hour < 21:
        return 'Dinner'

# Iterate over the incorrectly labeled records and update the order_type
for index, row in incorrect_df.iterrows():
    hour = int(row['Time'][:2])  # Extract the hour from the 'Time' column
    correct_order_type = categorize_order(hour)
    dirty_data.at[row['Index'], 'order_type'] = correct_order_type

"""In the bar graph below, we can now observe that all order types are correctly positioned within their respective time slots. This indicates that the misclassifications identified earlier have been rectified."""

# Group the data by hour and 'order_type', and count the occurrences
grouped_data_corrected = dirty_data.groupby(['hour', 'order_type']).size().unstack(fill_value=0)

# Plot the bar graph
plt.figure(figsize=(12, 8))
grouped_data_corrected.plot(kind='bar', stacked=True)
plt.xlabel('Hour')
plt.ylabel('Count')
plt.title('Corrected Order Types for Different Hours')
plt.xticks(rotation=45)
plt.legend(title='Order Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(axis='y')
plt.show()

# Drop the 'hour' column
dirty_data.drop(columns=['hour'], inplace=True)

"""<div class="alert alert-block alert-info">
    
#### **4.5.2. Examining `order_items`** <a class="anchor" name="exam_orderitems"></a>

For `order_items`, we first identify and print all unique order items present in the `order_items` column of the `dirty_data` DataFrame. The output shows a list of unique food items ordered, ensuring no duplicates are included:
"""

# Initialize an empty set to store unique items
unique_orderitems = set()

# Iterate over each row in the DataFrame
for index, row in dirty_data.iterrows():
    # Parse the string representation of the list and extract items
    items = eval(row['order_items'])
    # Iterate over each item tuple and add item to the set
    for item, _ in items:
        unique_orderitems.add(item)

# Print each unique item
for item in unique_orderitems:
    print(item)

"""The resulting bar graph shows the count of each order item for each order type ('Breakfast', 'Lunch', and 'Dinner'). From the output, it is clear that some items have outliers in terms of their categorization. For instance, majority of 'Burger' orders are placed during 'Lunch', but a few are incorrectly categorized as 'Breakfast' or 'Dinner'. Another example includes most 'Pancake' orders being placed during 'Breakfast', but some are miscategorized as 'Lunch' or 'Dinner'.

These outliers indicate potential errors in the categorization of order items based on their typical meal times. Correcting these misclassifications will help in providing a more accurate analysis of ordering patterns.
"""

# Convert 'order_items' from string to list
dirty_data['order_items'] = dirty_data['order_items'].apply(eval)

# Explode the list and split item and quantity
exploded_data = dirty_data.explode('order_items')
exploded_data[['item', 'quantity']] = pd.DataFrame(exploded_data['order_items'].tolist(), index=exploded_data.index)

# Group by 'order_type' and 'item' and sum the quantity
item_counts = exploded_data.groupby(['order_type', 'item'])['quantity'].sum().unstack('order_type').fillna(0)

# Define the desired order
order = ['Breakfast', 'Lunch', 'Dinner']

# Reindex the DataFrame
item_counts = item_counts.reindex(order, axis=1)

# Plot the DataFrame
item_counts.plot(kind='bar', figsize=(12, 6))
plt.title('Count of Order Items for Each Order Type')
plt.xlabel('Order Item')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Order Type')
plt.tight_layout()
plt.show()

# Breakfast: Cereal, Coffee, Eggs, Pancake
# Lunch: Burger, Chicken, Fries, Salad, Steak
# Dinner: Fish&Chips, Pasta, Salmon, Shrimp

"""Firstly, we create a dictionary named `menus`, mapping each `order_type` ('Breakfast', 'Lunch', 'Dinner') to a list of valid items for that meal type.

The code iterates over each row in the `dirty_data` DataFrame and retrieves the correct menu items based on the `order_type`. It then checks each item in the `order_items` column to see if it is part of the correct menu for the given `order_type`. For instance, if `order_type` is 'Breakfast', the only items that can be ordered should be cereal, coffee, eggs or pancake.

A new list `correct_items` is constructed, containing only those items that are present in the correct menu.

If the length of `correct_items` (the filtered list) does not match the length of the original `order_items` (the unfiltered list), it indicates that there were some items in the original list that were not valid for the given `order_type`. In such cases, the code updates the `order_items` column for that row with the `correct_items` list. Additionally, it marks the `checks` column for that row with the value `orderitems1` to indicate that a correction should be made later on.

For example, consider a row with `order_type` being 'Breakfast' and `order_items` consisting of ['Burger', 'Coffee', 'Eggs']:
*   The correct menu for 'Breakfast' includes ['Cereal', 'Coffee', 'Eggs', 'Pancake'].
*   'Burger' is not a valid item for 'Breakfast', so it gets removed.
*   The corrected order_items list will be ['Coffee', 'Eggs'].
*   The row is updated with this corrected list and marked for future reference in the `checks` column.


"""

# Define the correct menus
menus = {
    'Breakfast': ['Cereal', 'Coffee', 'Eggs', 'Pancake'],
    'Lunch': ['Burger', 'Chicken', 'Fries', 'Salad', 'Steak'],
    'Dinner': ['Fish&Chips', 'Pasta', 'Salmon', 'Shrimp']
}

# Iterate over each row in the DataFrame
for index, row in dirty_data.iterrows():
    # Get the correct menu for the order type
    correct_menu = menus[row['order_type']]
    # Check each item against the correct menu
    correct_items = [item for item in row['order_items'] if item[0] in correct_menu]

    # If any items were incorrect, update the 'checks' column and correct the 'order_items'
    if len(correct_items) != len(row['order_items']):
        dirty_data.at[index, 'checks'] = "orderitems1" # (37 wrong records)
        row['order_items'] = correct_items

# Convert 'order_items' back to string representation of list
dirty_data['order_items'] = dirty_data['order_items'].apply(str)

"""To extract the correct unit price for each item, we use the `missing_data` DataFrame. As the `order_items` column contains string representations of lists such as "[(item1, quantity1), (item2, quantity2)]", the `literal_eval` function is used to safely evaluate these string representations and convert them into actual Python list objects.

This transformation is necessary because working with actual list objects is significantly easier and more efficient for data manipulation and analysis.
"""

# Using missing_data file to extract unit price per item

df = missing_data

df['order_items'] = df['order_items'].apply(literal_eval)

"""In using the `missing_data` DataFrame to extract the unit price per item, this approach involves creating a matrix of item quantities and using linear algebra to solve for the unit prices.

The first step is to initialize data structures: `item_counts` will store the quantities of each item for every order, and `all_items` will keep track of all unique items across all orders. We then use a 'for' loop to iterate over all orders and collect each unique item into the `all_items` set.

For each order, we use another 'for' loop which creates a dictionary of item quantities. It then appends the quantity of each item (or 0 if the item is not in the order) to the `item_counts` list for that item.

The set of unique items (`all_items`) is later converted into a list called `item_list`. This ensures a consistent order of items, which is essential for creating the matrix. The `item_counts` dictionary, which stores lists of item quantities for each order, is converted into a 2D numpy array (`matrix`). Each row of this array represents an order, and each column represents the quantity of a specific item in that order. This structured representation allows us to analyze item quantities across all orders. Meanwhile, the `price_vector` contains the total order prices, representing the combined price for each order. This vector is used in the least squares computation to determine the unit prices of individual items.

Using `numpy.linalg.lstsq`, we solve the least squares problem to find the unit prices that best fit the given order prices. This result is stored in `unit_prices`, and these are mapped back to the items using `item_list`.

The final output consists of the unit prices for each item in a dictionary format.
"""

# Using missing_data file to extract unit price per item

# Dictionary to store item counts for each item across all orders
item_counts = defaultdict(list)
all_items = set()

# Collect all possible items
for items in df['order_items']:
    for item, _ in items:
        all_items.add(item)

# Fill in the matrix with quantities for each item
for _, row in df.iterrows():
    order_items = dict(row['order_items'])
    for item in all_items:
        item_counts[item].append(order_items.get(item, 0))

# Convert item counts to a matrix and prices to a vector
item_list = list(all_items)
matrix = np.array([item_counts[item] for item in item_list]).T
price_vector = np.array(df['order_price'])

# print("Matrix shape:", matrix.shape)
# print("Price vector shape:", price_vector.shape)

# Using numpy.linalg.lstsq to solve the least squares problem
unit_prices, residuals, rank, s = np.linalg.lstsq(matrix, price_vector, rcond=None)
item_prices = dict(zip(item_list, unit_prices))

# print("Item prices:", item_prices)
# print("Residuals:", residuals)

# Optionally format prices with two decimals in the output
formatted_prices = {item: f"${price:.2f}" for item, price in item_prices.items()}
print("Formatted item prices:", formatted_prices)

"""To facilitate easy access to prices when categorizing orders into their respective meal types, we organize the item prices into separate menus for each meal type.

This involves splitting the `formatted_prices` dictionary into separate menus based on the predefined meal categories ('Breakfast', 'Lunch', and 'Dinner'). For each meal category, a new dictionary is created (`breakfast_menu`, `lunch_menu`, `dinner_menu`) containing only the items and prices that belong to that specific meal. This is achieved using a dictionary comprehension that filters items based on their presence in the respective meal menus (`menus['Breakfast']`, `menus['Lunch']`, `menus['Dinner']`) which were defined in the `menus` dictionary previously.

Finally, a `menu_map` dictionary is created where each meal category ('Breakfast', 'Lunch', 'Dinner') is mapped to its corresponding menu dictionary (`breakfast_menu`, `lunch_menu`, `dinner_menu`).

The `menu_map` dictionary is printed below to display the mapping of each meal category to its respective menu. This organization simplifies menu management and aids in accurately categorizing orders during analysis.
"""

# Split formatted_prices into separate menus
breakfast_menu = {item: price for item, price in formatted_prices.items() if item in menus['Breakfast']}
lunch_menu = {item: price for item, price in formatted_prices.items() if item in menus['Lunch']}
dinner_menu = {item: price for item, price in formatted_prices.items() if item in menus['Dinner']}

# Map order types to menus
menu_map = {'Breakfast': breakfast_menu, 'Lunch': lunch_menu, 'Dinner': dinner_menu}

print(menu_map)

"""We then create a function, `correct_order_items`, to correct the order items for rows where the `checks` column indicates incorrect order items (i.e. `checks` = `orderitems1`).

The function starts by checking if the `checks` column for a particular row indicates incorrect order items. If the `checks` column value is `orderitems1`, it proceeds to correct the order items; otherwise, it returns the original order items.

For each invalid record, the correct menu corresponding to the order type from the `menu_map` is fetched. For instance, if `order_type` is 'Breakfast', the Breakfast menu is retrieved. Then, it parses the order items from the row and identifies the items that do not belong to the correct menu (`incorrect_items`).

For each incorrect item, the function calculates the missing price by subtracting the total price of all correct items from the total order price. It then divides the missing price by the quantity of the incorrect item to find the unit price.

Next, it searches for a correct item in the corresponding menu whose unit price closely matches the calculated unit price. If a correct item is found, it replaces the incorrect item with the correct one in the list of corrected items (`correct_items`).

After correcting all incorrect order items, the function returns the list of corrected items. If no incorrect items were found, it returns the original order items. This function is then applied to each row in the `dirty_data` DataFrame using the `apply` function.
"""

def correct_order_items(row):
    # If 'checks' column indicates incorrect order items
    if row['checks'] == "orderitems1":
        # Get the correct menu for the order type
        correct_menu = menu_map[row['order_type']]
        # Identify incorrect items
        order_items = eval(row['order_items'])
        incorrect_items = [item for item in order_items if item[0] not in correct_menu]
        # If there are incorrect items
        if incorrect_items:
            correct_items = []
            total_price = row['order_price']
            for item in order_items:
                if item in incorrect_items:
                    # Calculate the missing price
                    missing_price = total_price - sum(float(correct_menu[i[0]].replace('$', '')) * i[1] for i in order_items if i not in incorrect_items)
                    missing_price /= item[1]  # Divide by the quantity of the missing item
                    # Find the correct item based on the unit price
                    correct_item = next((menu_item for menu_item in correct_menu if abs(float(correct_menu[menu_item].replace('$', '')) - missing_price) < 0.01), None)
                    # If a correct item is found, add it to the list of correct items
                    if correct_item is not None:
                        correct_items.append((correct_item, item[1]))
                else:
                    # If the item is correct, add it to the list of correct items
                    correct_items.append(item)
            # Return the correct items
            return correct_items
    # If 'checks' column does not indicate incorrect order items, return the original order items
    return row['order_items']

# Apply the function to each row in the DataFrame
dirty_data['order_items'] = dirty_data.apply(correct_order_items, axis=1)

"""The distribution of corrected order items across different order types ('Breakfast', 'Lunch', 'Dinner') is visualized in the bar graph below, which provides a comprehensive view of the total count of each item across different order types. Each bar represents the count of an order item for each order type.

In the resulting bar graph, each order item now appears to belong to the correct order type, as per the defined menus for 'Breakfast', 'Lunch', and 'Dinner'. The corrected distribution aligns with the expected menu compositions:

* Breakfast: Cereal, Coffee, Eggs, Pancake
* Lunch: Burger, Chicken, Fries, Salad, Steak
* Dinner: Fish&Chips, Pasta, Salmon, Shrimp
"""

# Convert 'order_items' from string to list
dirty_data['order_items'] = dirty_data['order_items'].astype(str).apply(eval)

# Explode the list and split item and quantity
exploded_data = dirty_data.explode('order_items')
exploded_data[['item', 'quantity']] = pd.DataFrame(exploded_data['order_items'].tolist(), index=exploded_data.index)

# Group by 'order_type' and 'item' and sum the quantity
item_counts = exploded_data.groupby(['order_type', 'item'])['quantity'].sum().unstack('order_type').fillna(0)

# Define the desired order
order = ['Breakfast', 'Lunch', 'Dinner']

# Reindex the DataFrame
item_counts = item_counts.reindex(order, axis=1)

# Plot the DataFrame
item_counts.plot(kind='bar', figsize=(12, 6))
plt.title('Count of Order Items for Each Order Type (after correction)')
plt.xlabel('Order Item')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Order Type')
plt.tight_layout()
plt.show()

# Breakfast: Cereal, Coffee, Eggs, Pancake
# Lunch: Burger, Chicken, Fries, Salad, Steak
# Dinner: Fish&Chips, Pasta, Salmon, Shrimp

"""<div class="alert alert-block alert-info">
    
#### **4.5.3. Examining `order_price `** <a class="anchor" name="exam_orderprice"></a>

The next step is ensuring that the total order price is correct for each record.

Firstly, we define a `calculate_order_price` function which computes the total price for each order by iterating over the order items. It fetches the correct menu for the respective order type from the `menu_map`, then iterates through each item in the order, multiplying its price by the quantity and summing up the total price. The result is also rounded to two decimal places.

The function is then applied to each row in the `dirty_data` DataFrame, resulting in the addition of a new column `calculated_order_price` containing the calculated prices for each order.

Rows where the existing order price differs from the calculated order price are identified by comparing the values in the `order_price` and `calculated_order_price` columns. The indices of these rows are then stored in the `incorrect_price_rows` variable.

For rows with incorrect order prices, the values in the `order_price` column are replaced with the calculated order prices using the `loc` function. The `calculated_order_price` column, which was temporarily added for calculation purposes, is also dropped from the DataFrame to maintain data cleanliness.

This process ensures the accuracy and consistency of order prices in the dataset by reconciling any discrepancies between the recorded prices and the prices derived from the order items and menus.
"""

# Create a new column that copies the original order prices
dirty_data['original_order_price'] = dirty_data['order_price']

def calculate_order_price(row):
    # Initialize total price to 0
    total_price = 0.0

    # Get the correct menu for the order type
    correct_menu = menu_map[row['order_type']]

    # Iterate over each item in the order
    for item, quantity in row['order_items']:
        # Add the price of the item multiplied by the quantity to the total price
        total_price += float(correct_menu[item].replace('$', '')) * quantity

    # Return the total price
    return round(total_price, 2)

# Apply the function to each row in the DataFrame
dirty_data['calculated_order_price'] = dirty_data.apply(calculate_order_price, axis=1)

# Find the rows where the calculated order price is different from the existing order price (38 wrong records)
incorrect_price_rows = dirty_data[dirty_data['order_price'] != dirty_data['calculated_order_price']].index

# Replace the values in the 'order_price' column for these rows with the calculated order price
dirty_data.loc[incorrect_price_rows, 'order_price'] = dirty_data.loc[incorrect_price_rows, 'calculated_order_price']

# Update the 'checks' column for these rows with the value 'orderprice1'
dirty_data.loc[incorrect_price_rows, 'checks'] = 'orderprice1'

# Drop the 'calculated_price' column
dirty_data.drop(columns=['calculated_order_price'], inplace=True)

"""Based on the table below, the `original_order_price` column contains the incorrect price values, as it differs from the calculated `order_price` column. In this context, the `order_price` column is considered to be the correct representation of the total order prices."""

# Create a boolean mask where each value indicates whether the original and corrected 'order_price' values differ
mask = dirty_data['original_order_price'] != dirty_data['order_price']

# Use the mask to create a new dataframe that only contains the rows where the original and corrected 'order_price' values differ
corrected_rows = dirty_data[mask]

# Print the corrected rows
corrected_rows[['order_id', 'order_items', 'original_order_price', 'order_price']]

# Drop the 'original_order_price' column
dirty_data.drop(columns=['original_order_price'], inplace=True)

"""<div class="alert alert-block alert-info">
    
### **4.6. Examining `distance_to_customer_KM`** <a class="anchor" name="exam_distance"></a>
"""

# Before correcting the branch codes and distances, create new columns that copy the original values
dirty_data['original_branch_code'] = dirty_data['branch_code']
dirty_data['original_distance_to_customer_KM'] = dirty_data['distance_to_customer_KM']

"""<div class="alert alert-block alert-info">
    
#### **4.6.1. Create Graph** <a class="anchor" name="creat_graph"></a>

As we know that `distance_to_customer_KM` represents the shortest distance (measured in kilometres) between the branch and the customer, our initial step involves constructing a graph that captures the various pathways connecting customers and branches.

To accomplish this task, we'll utilize the NetworkX library to generate a graph where nodes are interconnected by edges, each assigned specific weights. The edges and their weights are derived from the `edges` dataframe, while the nodes themselves are extracted from the `nodes` dataframe.
"""

# Create a directed graph from the DataFrame
G = nx.Graph()
for idx, row in edges.iterrows():
    G.add_edge(row['u'], row['v'], weight=row['distance(m)'])

"""<div class="alert alert-block alert-info">
    
#### **4.6.2. Assign node to customers & branches** <a class="anchor" name="assign_node"></a>

In our constructed graph, each node is defined by its respective latitude and longitude coordinates. Since each customer and branch location is also characterized by latitude and longitude coordinates, we can leverage this information to precisely position them within the graph. This approach enables us to accurately pinpoint the locations of both customers and branches on the graph.
"""

# Function to find node based on latitude and longitude
def find_node(lat, lon):
    # Find the row in node_df that matches the latitude and longitude
    matched_row = nodes[(nodes['lat'] == lat) & (nodes['lon'] == lon)]
    # If there is a match, return the node, otherwise return None
    if not matched_row.empty:
        return matched_row['node'].values[0]
    return None

# Apply the function to each row in dirty_data
dirty_data['node'] = dirty_data.apply(lambda row: find_node(row['customer_lat'], row['customer_lon']), axis=1)
branches['node'] = branches.apply(lambda row: find_node(row['branch_lat'], row['branch_lon']), axis=1)

dirty_data.head()

branches.head()

"""<div class="alert alert-block alert-info">
    
#### **4.6.3. Find shortest distance** <a class="anchor" name="find_distance"></a>

We'll define a function named `cal_distances` tasked with calculating the shortest path distances from a set of branch nodes to customer nodes utilizing Dijkstra's algorithm. These distances will be stored as an array, `[distance to NS, distance to TP, distance to BK]`, in a new column. This approach allows us to leverage these distances to identify anomalies in either `distance_to_customer_KM` or `branch_code`.
"""

# Creating a dictionary for quick node lookup from branches DataFrame
node_lookup = branches.set_index('branch_code')['node'].to_dict()

def cal_distances(row):
    try:
        # Initialize an empty list to store distances
        distance_array = []

        # Iterate over all branch nodes
        for branch_node in node_lookup.values():
            # Check if the branch_node is None (branch code not found)
            if branch_node is None:
                return None

            # Get the customer node from the current row
            customer_node = row['node']

            # Calculate the shortest path distance using Dijkstra's algorithm and convert to kilometers
            calculated_distance = nx.dijkstra_path_length(G, branch_node, customer_node, weight='weight') / 1000

            # Append the calculated distance to the list
            distance_array.append(calculated_distance)

        # Return the list of distances
        return distance_array

    # Handle the case where there is no path between nodes
    except nx.NetworkXNoPath:
        return None

    # Handle any other exceptions and print an error message
    except Exception as e:
        print(f"Error calculating path for row {row}: {e}")
        return None

# Applying the cal_distances function to dirty_data DataFrame
dirty_data['distance(NS,TP,BK)'] = dirty_data.apply(cal_distances, axis=1)

dirty_data.head(10)

"""<div class="alert alert-block alert-info">
    
#### **4.6.4. Examining `distance_to_customer_KM` & `branch_code`** <a class="anchor" name="exam_distnce_branch"></a>

Utilizing the calculated distance array, `[distance to NS, distance to TP, distance to BK]`, which represents the distances from the customer to all three branches, we can rectify inconsistencies in the `distance_to_customer_KM` and `branch_code` values. This involves comparing the `distance_to_customer_KM` with the three elements in the distance array. If the `distance_to_customer_KM` matches one of the elements but the `branch_code` is different, we update the `branch_code` accordingly.

Conversely, in cases where the `distance_to_customer_KM` does not match any of the elements in the distance array, we then set the `distance_to_customer_KM` based on the correct `branch_code`.
"""

def correct_distance_and_branch(row):
    # Get the distances and branch
    distances = row['distance(NS,TP,BK)']
    branch = row['branch_code']
    distance_to_customer = row['distance_to_customer_KM']

    # Initialize a flag to indicate whether a correction was made
    correction_made = False

    # Check if the distance to customer is equal to any of the distances
    if distance_to_customer in distances:
        # If the distance to customer is equal to the first distance, the branch should be 'NS'
        if distance_to_customer == distances[0] and branch != 'NS':
            row['branch_code'] = 'NS'
            correction_made = True
        # If the distance to customer is equal to the second distance, the branch should be 'TP'
        elif distance_to_customer == distances[1] and branch != 'TP':
            row['branch_code'] = 'TP'
            correction_made = True
        # If the distance to customer is equal to the third distance, the branch should be 'BK'
        elif distance_to_customer == distances[2] and branch != 'BK':
            row['branch_code'] = 'BK'
            correction_made = True
    else:
        # If the distance to customer is not equal to any of the distances, set it to the correct distance based on the branch
        if branch == 'NS':
            row['distance_to_customer_KM'] = distances[0]
        elif branch == 'TP':
            row['distance_to_customer_KM'] = distances[1]
        elif branch == 'BK':
            row['distance_to_customer_KM'] = distances[2]
        correction_made = True

    # If a correction was made, update the 'checks' column
    if correction_made:
        row['checks'] = 'distance1'

    return row

# Apply the function to each row in the DataFrame
dirty_data = dirty_data.apply(correct_distance_and_branch, axis=1)

# Create a boolean mask where each value indicates whether the original and corrected 'branch_code' or 'distance_to_customer_KM' values differ
mask = (dirty_data['original_branch_code'] != dirty_data['branch_code']) | (dirty_data['original_distance_to_customer_KM'] != dirty_data['distance_to_customer_KM'])

# Use the mask to create a new dataframe that only contains the rows where the original and corrected 'branch_code' or 'distance_to_customer_KM' values differ
corrected_rows = dirty_data[mask]

# Print the corrected rows
corrected_rows[['order_id', 'original_branch_code', 'branch_code', 'original_distance_to_customer_KM', 'distance_to_customer_KM', 'distance(NS,TP,BK)']]

# Drop the 'node' and 'distance(NS,TP,BK)' columns
dirty_data.drop(columns=['original_branch_code', 'original_distance_to_customer_KM', 'node', 'distance(NS,TP,BK)'], inplace=True)

"""<div class="alert alert-block alert-info">
    
### **4.7. Examining `customerHasloyalty?`** <a class="anchor" name="exam_loyalty"></a>

Since we know that if `customerHasloyalty?` is 1, the delivery fee is halved; we can use this information to spot anomalies in the `customerHasloyalty?` variable. Our approach involves using the linear model to predict the `delivery_fee` and comparing it with the given delivery fee. If the given delivery fee and predicted delivery fee are similar, there is no anomaly in `customerHasloyalty?`.

However, consider the following scenarios:
* If `customerHasloyalty?` is 1 and the given delivery fee is 12, but the predicted delivery fee is 6, this indicates an anomaly. The `customerHasloyalty?` should be 0 because, if `customerHasloyalty?` was initially set to 0, the predicted delivery fee would be around 12 instead, matching the given delivery fee.
* Conversely, if `customerHasloyalty?` is 0 and the given delivery fee is 12, but the predicted delivery fee is 24, this also indicates an anomaly. The `customerHasloyalty?` should be 1 because, with `customerHasloyalty?` set to 1, the predicted delivery fee would have been halved to 12, matching the given delivery fee.

This approach is effective because our linear model takes `customerHasloyalty?` into consideration when making predictions. By comparing the predicted delivery fee with the actual delivery fee (which has no errors), we can identify and correct anomalies in the `customerHasloyalty?` variable.

<div class="alert alert-block alert-info">
    
#### **4.7.1. Prepare data for prediction** <a class="anchor" name="prepare_prediction_dirty"></a>

Here, we follow the same steps we used when preparing the data for training the linear model.
"""

to_predict = dirty_data.copy()
to_predict.head()

"""We convert the `date` to `0` for weekdays and `1` for weekends using the following code:"""

# # Define a function to categorize day into weekday and weekend
def change_date(date):
  # Ensure the 'date' column is in datetime format
  date = pd.to_datetime(date)
  if date.dayofweek > 4:  # weekend
    return 1
  return 0  # weekday

# Apply transformation to categorize dates as 0 for weekdays and 1 for weekends
to_predict['date'] = to_predict['date'].apply(change_date)

"""We convert the `time` to `0` for morning, `1` for afternoon, and `2` for evening using the following code:"""

# Define a function to categorize time into morning, afternoon, and evening
def categorize_time(time):
    # Ensure the 'time' column is recognized as datetime type
    time = pd.to_datetime(time, format='%H:%M:%S').time()
    if datetime.time(8, 0) <= time <= datetime.time(12, 0):
        return 0  # Morning
    elif datetime.time(12, 0, 1) <= time <= datetime.time(16, 0):
        return 1  # Afternoon
    elif datetime.time(16, 0, 1) <= time <= datetime.time(20, 0):
        return 2  # Evening

# Apply the categorization function to the 'time' column
to_predict['time'] = to_predict['time'].apply(categorize_time)

"""We update the `delivery_fee` to its original price, considering whether `customerHasloyalty?` is true, using the following code:"""

# Define a function to update the delivery_fee to its original price
def update_fee(row):
    if row['customerHasloyalty?'] == 1:
        return row['delivery_fee'] * 2
    return row['delivery_fee']


# Apply the categorization function to the 'branch_code' column
to_predict['original_fee'] = to_predict.apply(update_fee, axis=1)

"""<div class="alert alert-block alert-info">
    
#### **4.7.2. Predict `Delivery_fee`** <a class="anchor" name="predict_delivery_fee_dirty"></a>

To make predictions, we followed the same steps we used when predicting missing values.
"""

# Separate the DataFrame based on branch_code values
to_predict_1 = to_predict[to_predict['branch_code'] == 'NS'].copy()
to_predict_2 = to_predict[to_predict['branch_code'] == 'TP'].copy()
to_predict_3 = to_predict[to_predict['branch_code'] == 'BK'].copy()

# Prepare features
features_branch_10 = [x for x in to_predict_1.columns if x not in ['order_id', 'order_items', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'delivery_fee', 'checks', 'original_fee']]
features_branch_20 = [x for x in to_predict_2.columns if x not in ['order_id', 'order_items', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'delivery_fee', 'checks', 'original_fee']]
features_branch_30 = [x for x in to_predict_3.columns if x not in ['order_id', 'order_items', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'delivery_fee', 'checks', 'original_fee']]

# predict
X_unknown1 = to_predict_1[features_branch_10]
to_predict_1['predicted_delivery_fee'] = linearReg_1.predict(X_unknown1)

X_unknown2 = to_predict_2[features_branch_20]
to_predict_2['predicted_delivery_fee'] = linearReg_2.predict(X_unknown2)

X_unknown3 = to_predict_3[features_branch_30]
to_predict_3['predicted_delivery_fee'] = linearReg_3.predict(X_unknown3)

# Join the DataFrames
combined_predicted_df = pd.concat([to_predict_1, to_predict_2, to_predict_3], ignore_index=True)

dirty_data = pd.merge(dirty_data, combined_predicted_df[['order_id', 'predicted_delivery_fee']], on='order_id', how='left')

dirty_data.info()

"""<div class="alert alert-block alert-info">
    
#### **4.7.3. Examining `customerHasloyalty?`** <a class="anchor" name="exam_loyalty"></a>
"""

# Before correcting the customerHasloyalty, create new columns that copy the original values
dirty_data['original_customerHasloyalty?'] = dirty_data['customerHasloyalty?']

"""Here, we define a `check_loyalty` function to determine whether the `predicted_delivery_fee` is significantly different from the original `delivery_fee`.  Given the linear model's accuracy of approximately 97%, we set a threshold of 20%.

If the `predicted_delivery_fee` exceeds the `delivery_fee` by more than 20%, it is considered doubled. Conversely, if it falls below the `delivery_fee` by more than 20%, it is considered halved.

The function computes lower and upper thresholds by multiplying the delivery fee by 0.8 and 1.2, respectively. It then compares the predicted delivery fee against these thresholds. If the predicted delivery fee exceeds the upper threshold, indicating it's more than 20% higher than the original price, the function returns 1, suggesting loyalty. Otherwise, it returns 0, indicating no loyalty.

We then apply the `check_loyalty` function to each row of the `dirty_data` DataFrame. It computes the `loyalty_check` value for each row based on the logic defined in the `check_loyalty` function. The resulting `loyalty_check` values are stored in a new column called `loyalty_check` in the `dirty_data` DataFrame.
"""

def check_loyalty(row):
  # define threshold
    lower_threshold = row['delivery_fee'] * 0.8
    upper_threshold = row['delivery_fee'] * 1.2
    # check whether predicted_delivery_fee is within the lower or upper threshold
    if row['predicted_delivery_fee'] > upper_threshold:
        return 1
    else:
        return 0

dirty_data['loyalty_check'] = dirty_data.apply(check_loyalty, axis=1)

"""The next step involves constructing a boolean mask termed `mask`, filtering rows where the `loyalty_check` column is not NaN and where `customerHasloyalty?` is not equal to `loyalty_check`. Subsequently, it applies this mask to select rows meeting these conditions.

For these selected rows, it updates the `customerHasloyalty?` column with the values from the `loyalty_check` column, aligning them with the loyalty check.
"""

# Select rows where 'loyalty_check' is not NaN and 'customerHasloyalty?' is not equal to 'loyalty_check'
mask = dirty_data['loyalty_check'].notna() & (dirty_data['customerHasloyalty?'] != dirty_data['loyalty_check'])

# Change the value of 'customerHasloyalty?' to the value of 'loyalty_check' for the selected rows
dirty_data.loc[mask, 'customerHasloyalty?'] = dirty_data.loc[mask, 'loyalty_check']

# Update the 'checks' column for the rows that were changed
dirty_data.loc[mask, 'checks'] = 'loyalty1'

"""The table displays corrected rows where the original and corrected values of `customerHasloyalty?` differ. `original_customerHasloyalty?` represents the value before the change, while `customerHasloyalty?` reflects the value after the adjustment.

Rows with delivery fees close to their predicted counterparts have their `customerHasloyalty?` changed from '1' to '0'. Conversely, rows where the delivery fee is roughly half of the predicted fee see their `customerHasloyalty?` changed from '0' to '1'.
"""

# Use the mask to create a new dataframe that only contains the rows where the original and corrected 'branch_code' or 'distance_to_customer_KM' values differ
corrected_rows = dirty_data[mask]

# Print the corrected rows
corrected_rows[['order_id', 'original_customerHasloyalty?', 'customerHasloyalty?', 'delivery_fee', 'predicted_delivery_fee']]

"""To streamline the DataFrame for download, we removed redundant columns."""

# Drop the 'predicted_delivery_fee', 'checks' and 'loyalty_check' columns
final_dirty_data = dirty_data.drop(columns=['loyalty_check', 'checks', 'predicted_delivery_fee', 'original_customerHasloyalty?'])

"""-------------------------------------

<div class="alert alert-block alert-warning">

## **5.  Outlier Data** <a class="anchor" name="outlier_data"></a>

</div>

There are two types of outliers: univariate and multivariate.

A univariate outlier is an observation in a dataset that significantly deviates from the rest of the data based on a single variable. These outliers can be identified by analyzing the distribution of values in that particular variable.

In contrast, a multivariate outlier is an observation that deviates significantly from the general pattern formed by multiple variables considered together. Unlike univariate outliers, which are detected based on a single variable, multivariate outliers are identified based on the combined behavior of multiple variables.

In our case, the `delivery_fee` variable represents a multivariate outlier, hence we will utilize a linear model to understand these deviations by fitting the model to expected relationships and identifying points that do not conform.

**Steps for Detecting Multivariate Outliers Using a Linear Model:**

1. Train the Linear Model: Utilize the trained model to predict missing values.

2. Calculate Residuals: Residuals, which are the differences between actual and predicted values, assist in outlier identification.

3. Identify Outliers Using IQR on Residuals: the Interquartile Range (IQR) method can be applied to the residuals to classify outliers. Here's how:

  1. Calculate the IQR of Residuals:
    * Compute the first quartile (`Q1`) and the third quartile (`Q3`).
    * Determine the `IQR` as `Q3 - Q1`.
    * Define outlier boundaries as `Q1 - 1.5 * IQR` for the lower bound and `Q3 + 1.5 * IQR` for the upper bound.

  2. Classify Outliers:
    * Any residual outside these boundaries is classified as an outlier.

<div class="alert alert-block alert-info">
    
### **5.1. Examining Outlier Data** <a class="anchor" name="exam_outlier_data"></a>

Observing 500 rows with no missing values, we recognize the need to transform the dataset into a suitable format for prediction. To facilitate this transformation, we will duplicate this DataFrame into a new DataFrame named `df_outlier`.
"""

outlier_data.info()

df_outlier = outlier_data.copy()
df_outlier.info()

"""<div class="alert alert-block alert-info">
    
### **5.2. Data Transformation & Prediction** <a class="anchor" name="transform_data_prediction"></a>

Next, we'll prepare the dataset for prediction by transforming it into a suitable format. This involves organizing features such as `date`, `time`, and `original_fee`, and then using them to predict the `delivery_fee` for each `branch_code`.
"""

df_outlier['date'] = df_outlier['date'].apply(change_date)
df_outlier['time'] = df_outlier['time'].apply(categorize_time)
df_outlier['original_fee'] = df_outlier.apply(update_fee, axis=1)
df_outlier.head()

"""Selecting features specific to each branch, we proceed with predicting the `delivery_fee`.

This involves segregating the DataFrame based on `branch_code` values into `outlier_1`, `outlier_2`, and `outlier_3` for branches 'NS', 'TP', and 'BK' respectively. After preparing the features, prediction is performed for each branch using linear regression models `linearReg_1`, `linearReg_2`, and `linearReg_3`.
"""

# Separate the DataFrame based on branch_code values
outlier_1 = df_outlier[df_outlier['branch_code'] == 'NS'].copy()
outlier_2 = df_outlier[df_outlier['branch_code'] == 'TP'].copy()
outlier_3 = df_outlier[df_outlier['branch_code'] == 'BK'].copy()

# Prepare features
features_outlier_10 = [x for x in outlier_1.columns if x not in ['order_id', 'order_items', 'delivery_fee', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'original_fee']]
features_outlier_20 = [x for x in outlier_2.columns if x not in ['order_id', 'order_items', 'delivery_fee', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'original_fee']]
features_outlier_30 = [x for x in outlier_3.columns if x not in ['order_id', 'order_items', 'delivery_fee', 'customer_lat', 'customer_lon', 'branch_code', 'order_type', 'customerHasloyalty?', 'order_price', 'original_fee']]

# predict
XO_unknown1 = outlier_1[features_outlier_10]
outlier_1['predicted_delivery_fee'] = linearReg_1.predict(XO_unknown1)

XO_unknown2 = outlier_2[features_outlier_20]
outlier_2['predicted_delivery_fee'] = linearReg_2.predict(XO_unknown2)

XO_unknown3 = outlier_3[features_outlier_30]
outlier_3['predicted_delivery_fee'] = linearReg_3.predict(XO_unknown3)

"""We then merge the three DataFrames corresponding to each `branch_code` back into the original DataFrame, `outlier_data`, while adding an additional column, `predicted_delivery_fee`, to the `outlier_data` DataFrame."""

combined_predicted_outlier_df = pd.concat([outlier_1, outlier_2, outlier_3], ignore_index=True)
outlier_data_f = pd.merge(outlier_data, combined_predicted_outlier_df[['order_id', 'predicted_delivery_fee']], on='order_id', how='left')

outlier_data_f.info()

"""The following code restores the `delivery_fee` based on the customer's loyalty status `customerHasloyalty?`."""

outlier_data_f['predicted_delivery_fee'] = outlier_data_f.apply(restore_fee, axis=1)

outlier_data_f.head(10)

"""Here, we generate a plot comparing the actual and predicted `delivery_fee` values, revealing the presence of outliers that require removal."""

plt.figure(figsize=(10, 6))
plt.scatter(outlier_data_f['delivery_fee'], outlier_data_f['predicted_delivery_fee'], color='blue')
plt.plot([min(outlier_data_f['delivery_fee']), max(outlier_data_f['delivery_fee'])], [min(outlier_data_f['delivery_fee']), max(outlier_data_f['delivery_fee'])], color='red', linestyle='--')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Delivery_fee With Outliers')
plt.show()

"""<div class="alert alert-block alert-info">
    
### **5.3. Detect, Visualise and Remove Outliers** <a class="anchor" name="detect_visualise_remove_outlier"></a>

Here, we compute the residual for each row in the dataset.
"""

def cal_residual(row):
    return row['predicted_delivery_fee'] - row['delivery_fee']

# Apply the function to each row in dirty_data
outlier_data_f['residual'] = outlier_data_f.apply(cal_residual, axis=1)
outlier_data_f.head(10)

"""Then, we split the `outlier_data` into three dataframes for each `branch_code` to detect outliers."""

# Separate the DataFrame based on branch_code values
find_outlier_1 = outlier_data_f[outlier_data_f['branch_code'] == 'NS'].copy()
find_outlier_2 = outlier_data_f[outlier_data_f['branch_code'] == 'TP'].copy()
find_outlier_3 = outlier_data_f[outlier_data_f['branch_code'] == 'BK'].copy()

"""For each `branch_code`, we compute the Interquartile Range (IQR) for the residuals and establish lower and upper bounds to detect outliers. Subsequently, we identify and count the outliers based on these bounds. The residuals are visualized using a boxplot, highlighting the outlier thresholds for easier interpretation. Finally, we create a new dataframe that excludes the detected outliers.

<div class="alert alert-block alert-info">
    
#### **5.3.1 Branch `NS`** <a class="anchor" name="ns"></a>
"""

# Calculate IQR
Q1 = np.percentile(find_outlier_1['residual'], 25)
Q3 = np.percentile(find_outlier_1['residual'], 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers_iqr = (find_outlier_1['residual'] < lower_bound) | (find_outlier_1['residual'] > upper_bound)
outlier_indices_iqr_1 = np.where(outliers_iqr)[0]

print(f"Number of outliers detected by IQR: {len(outlier_indices_iqr_1)}")

# Visualization
plt.figure(figsize=(14, 7))

plt.plot()
sns.boxplot(x=find_outlier_1['residual'])
plt.axvline(lower_bound, color='r', linestyle='--')
plt.axvline(upper_bound, color='r', linestyle='--')
plt.title('Residuals with IQR Threshold')

plt.tight_layout()
plt.show()

# Remove outliers from the DataFrame
find_outlier_1_cleaned = find_outlier_1.loc[~outliers_iqr]

"""Here, we visualize the Actual vs Predicted `delivery_fee` for branch `NS` both before and after removing outliers.

On the right side, we observe approximately 10 outliers that were subsequently removed, as demonstrated by the plot on the left.
"""

# Create a figure with two subplots
fig, ax = plt.subplots(1, 2, figsize=(20, 6))

# Plot for Actual vs Predicted Without Outliers
ax[0].scatter(find_outlier_1_cleaned['delivery_fee'], find_outlier_1_cleaned['predicted_delivery_fee'], color='blue')
ax[0].plot([min(find_outlier_1_cleaned['delivery_fee']), max(find_outlier_1_cleaned['delivery_fee'])],
           [min(find_outlier_1_cleaned['delivery_fee']), max(find_outlier_1_cleaned['delivery_fee'])],
           color='red', linestyle='--')
ax[0].set_xlabel('Actual')
ax[0].set_ylabel('Predicted')
ax[0].set_title('Actual vs Predicted Without Outliers')

# Plot for Actual vs Predicted With Outliers
ax[1].scatter(find_outlier_2['delivery_fee'], find_outlier_2['predicted_delivery_fee'], color='blue')
ax[1].plot([min(find_outlier_2['delivery_fee']), max(find_outlier_2['delivery_fee'])],
           [min(find_outlier_2['delivery_fee']), max(find_outlier_2['delivery_fee'])],
           color='red', linestyle='--')
ax[1].set_xlabel('Actual')
ax[1].set_ylabel('Predicted')
ax[1].set_title('Actual vs Predicted With Outliers')

# Adjust layout
plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **5.3.2 Branch `TP`** <a class="anchor" name="tp"></a>
"""

# Calculate IQR
Q1 = np.percentile(find_outlier_2['residual'], 25)
Q3 = np.percentile(find_outlier_2['residual'], 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers_iqr = (find_outlier_2['residual'] < lower_bound) | (find_outlier_2['residual'] > upper_bound)
outlier_indices_iqr_2 = np.where(outliers_iqr)[0]

print(f"Number of outliers detected by IQR: {len(outlier_indices_iqr_2)}")

# Visualization
plt.figure(figsize=(14, 7))

plt.plot()
sns.boxplot(x=find_outlier_2['residual'])
plt.axvline(lower_bound, color='r', linestyle='--')
plt.axvline(upper_bound, color='r', linestyle='--')
plt.title('Residuals with IQR Threshold')

plt.tight_layout()
plt.show()

# Remove outliers from the DataFrame
find_outlier_2_cleaned = find_outlier_2.loc[~outliers_iqr]

"""Here, we plot the Actual vs Predicted `delivery_fee` for branch `TP` before and after the removal of outliers.

On the right, approximately 7 outliers are evident, which have been removed, as indicated by the plot on the left.
"""

# Create a figure with two subplots
fig, ax = plt.subplots(1, 2, figsize=(20, 6))

# Plot for Actual vs Predicted Without Outliers
ax[0].scatter(find_outlier_2_cleaned['delivery_fee'], find_outlier_2_cleaned['predicted_delivery_fee'], color='blue')
ax[0].plot([min(find_outlier_2_cleaned['delivery_fee']), max(find_outlier_2_cleaned['delivery_fee'])],
           [min(find_outlier_2_cleaned['delivery_fee']), max(find_outlier_2_cleaned['delivery_fee'])],
           color='red', linestyle='--')
ax[0].set_xlabel('Actual')
ax[0].set_ylabel('Predicted')
ax[0].set_title('Actual vs Predicted Without Outliers')

# Plot for Actual vs Predicted With Outliers
ax[1].scatter(find_outlier_2['delivery_fee'], find_outlier_2['predicted_delivery_fee'], color='blue')
ax[1].plot([min(find_outlier_2['delivery_fee']), max(find_outlier_2['delivery_fee'])],
           [min(find_outlier_2['delivery_fee']), max(find_outlier_2['delivery_fee'])],
           color='red', linestyle='--')
ax[1].set_xlabel('Actual')
ax[1].set_ylabel('Predicted')
ax[1].set_title('Actual vs Predicted With Outliers')

# Adjust layout
plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **5.3.3 Branch `BK`** <a class="anchor" name="bk"></a>
"""

# Calculate IQR
Q1 = np.percentile(find_outlier_3['residual'], 25)
Q3 = np.percentile(find_outlier_3['residual'], 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers_iqr = (find_outlier_3['residual'] < lower_bound) | (find_outlier_3['residual'] > upper_bound)
outlier_indices_iqr_3 = np.where(outliers_iqr)[0]

print(f"Number of outliers detected by IQR: {len(outlier_indices_iqr_3)}")

# Visualization
plt.figure(figsize=(14, 7))

plt.plot()
sns.boxplot(x=find_outlier_3['residual'])
plt.axvline(lower_bound, color='r', linestyle='--')
plt.axvline(upper_bound, color='r', linestyle='--')
plt.title('Residuals with IQR Threshold')

plt.tight_layout()
plt.show()

# Remove outliers from the DataFrame
find_outlier_3_cleaned = find_outlier_3.loc[~outliers_iqr]

"""Here, we visualize the Actual vs Predicted `delivery_fee` for branch `BK` both before and after removing outliers.

On the right, there are approximately 14 outliers evident, which have been removed, as depicted by the plot on the left.
"""

# Create a figure with two subplots
fig, ax = plt.subplots(1, 2, figsize=(20, 6))

# Plot for Actual vs Predicted Without Outliers
ax[0].scatter(find_outlier_1_cleaned['delivery_fee'], find_outlier_1_cleaned['predicted_delivery_fee'], color='blue')
ax[0].plot([min(find_outlier_1_cleaned['delivery_fee']), max(find_outlier_1_cleaned['delivery_fee'])],
           [min(find_outlier_1_cleaned['delivery_fee']), max(find_outlier_1_cleaned['delivery_fee'])],
           color='red', linestyle='--')
ax[0].set_xlabel('Actual')
ax[0].set_ylabel('Predicted')
ax[0].set_title('Actual vs Predicted Without Outliers')

# Plot for Actual vs Predicted With Outliers
ax[1].scatter(find_outlier_2['delivery_fee'], find_outlier_2['predicted_delivery_fee'], color='blue')
ax[1].plot([min(find_outlier_2['delivery_fee']), max(find_outlier_2['delivery_fee'])],
           [min(find_outlier_2['delivery_fee']), max(find_outlier_2['delivery_fee'])],
           color='red', linestyle='--')
ax[1].set_xlabel('Actual')
ax[1].set_ylabel('Predicted')
ax[1].set_title('Actual vs Predicted With Outliers')

# Adjust layout
plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **5.3.4 Merge Dataframe** <a class="anchor" name="merge_dataframe"></a>

After removing outliers from all three branches, we consolidate them into a single dataframe.
"""

print("Total number of outliers remove:", len(outlier_indices_iqr_1) + len(outlier_indices_iqr_2) + len(outlier_indices_iqr_3))
# Join the DataFrames
cleaned_outlier_df = pd.concat([find_outlier_1_cleaned, find_outlier_2_cleaned, find_outlier_3_cleaned], ignore_index=True)
# cleaned_outlier_df.drop(columns=['predicted_delivery_fee', 'residual'])
# cleaned_outlier_df.info()

"""We once again plot the actual vs predicted `delivery_fee` and see that there are no noticeable outliers."""

# Create a figure with two subplots
fig, ax = plt.subplots(1, 2, figsize=(20, 6))

# Plot for Actual vs Predicted Without Outliers
ax[0].scatter(cleaned_outlier_df['delivery_fee'], cleaned_outlier_df['predicted_delivery_fee'], color='blue')
ax[0].plot([min(cleaned_outlier_df['delivery_fee']), max(cleaned_outlier_df['delivery_fee'])],
           [min(cleaned_outlier_df['delivery_fee']), max(cleaned_outlier_df['delivery_fee'])],
           color='red', linestyle='--')
ax[0].set_xlabel('Actual')
ax[0].set_ylabel('Predicted')
ax[0].set_title('Actual vs Predicted Without Outliers (all branches combined)')

# Plot for Actual vs Predicted With Outliers
ax[1].scatter(outlier_data_f['delivery_fee'], outlier_data_f['predicted_delivery_fee'], color='blue')
ax[1].plot([min(outlier_data_f['delivery_fee']), max(outlier_data_f['delivery_fee'])],
           [min(outlier_data_f['delivery_fee']), max(outlier_data_f['delivery_fee'])],
           color='red', linestyle='--')
ax[1].set_xlabel('Actual')
ax[1].set_ylabel('Predicted')
ax[1].set_title('Actual vs Predicted With Outliers (all branches combined)')

# Adjust layout
plt.tight_layout()
plt.show()

"""We dropped unnecessary columns to prepare the dataframe for download."""

final_outlier_df = cleaned_outlier_df.drop(columns=['predicted_delivery_fee', 'residual'])
final_outlier_df.info()

"""-------------------------------------

<div class="alert alert-block alert-warning">

## <u>**Task 2**<u>  <a class="anchor" name="task2"></a>
    
</div>

<div class="alert alert-block alert-info">
    
## **6. Data Reshaping** <a class="anchor" name="data_reshaping"></a>

<div class="alert alert-block alert-info">
    
### **6.1. Exploring suburb_info data** <a class="anchor" name="explore_task2_data"></a>

In this first step, we inspect the initial rows of the `suburb_info` dataset to understand the structure and types of data available, including information about suburbs such as the number of houses, units, population, percentage of Australian-born residents, median income, and median house prices. This exploration helps in identifying the relevant features that would be used later on for the regression analysis.
"""

suburb_info.head()

"""We also generate summary statistics for the `suburb_info` dataset which includes counts, unique values, top frequencies, means, standard deviations, and percentile distributions for each column. This output reveals that the numeric variables, such as `number_of_houses`, `number_of_units`, and `population`, are on different scales, with significant variation in their values.

For example, `number_of_houses` ranges from 283 to 23,338, whereas `population` ranges from 170 to 54,005. Identifying these discrepancies in scale underscores the necessity of scaling these variables before performing regression analysis to ensure that each feature contributes equally to the model.
"""

# Summary statistics

suburb_info.describe(include='all')

"""We also examine the data types of each column in the `suburb_info` dataset. It is observed that the `aus_born_perc`, `median_income`, and `median_house_price` columns are incorrectly recognized as object types due to the presence of percentage signs and dollar signs."""

# Check data types
suburb_info.dtypes

"""To rectify this, we remove these symbols and convert the columns to floating-point numbers. Specifically, we strip the percentage sign from `aus_born_perc`, and the dollar sign and commas from `median_income` and `median_house_price`.

These variables need to be in numeric format as we are preparing to develop a linear model to predict the `median_house_price` using the following attributes: `number_of_houses`, `number_of_units`, `population`, `aus_born_perc`, and `median_income`. This ensures that each attribute can be correctly processed and analyzed to understand their effects on predicting `median_house_price`.

The transformed dataset can be seen as below:
"""

# Remove percentage sign and convert to float
suburb_info['aus_born_perc'] = suburb_info['aus_born_perc'].str.rstrip('%').astype('float')

# Remove dollar sign, comma and convert to float
suburb_info['median_income'] = suburb_info['median_income'].str.replace('$', '').str.replace(',', '').astype('float')
suburb_info['median_house_price'] = suburb_info['median_house_price'].str.replace('$', '').str.replace(',', '').astype('float')

suburb_info.head()

"""We also check for missing values in the `suburb_info` dataset. The output indicates that there are no missing values in any of the columns."""

# Check for missing values
suburb_info.isnull().sum()

"""After applying the necessary changes, we re-evaluate the data types of each column in the `suburb_info` dataset. Now, all numeric variables (`number_of_houses`, `number_of_units`, `aus_born_perc`, `median_income`, `median_house_price`, `population`) are appropriately represented as either integer or float types, while categorical variables (`suburb`, `municipality`) remain as object types."""

# Check data types
suburb_info.dtypes

"""When visualizing the distributions of numeric variables in the `suburb_info` dataset, we find the following observations:

* Both `number_of_houses` and `number_of_units` exhibit very high right-skewness, indicating that most suburbs have relatively low numbers of houses and units, with a few suburbs having significantly higher numbers. These variables also have a considerable number of outliers present on the right side of their distributions, suggesting extreme values that deviate from the typical pattern.
* Similarly, the distributions of `median_house_price` and `population` are moderately right-skewed, suggesting that most suburbs have lower median house prices and populations, with a few suburbs having higher values. These variables also have some outliers on the right side, indicating unusually high house prices or population sizes in certain suburbs.
* The distribution of `aus_born_perc` is moderately left-skewed, indicating that a majority of suburbs have higher percentages of Australian-born residents, while fewer suburbs have lower percentages. Notably, there is an outlier present on the left side of the distribution, representing a suburb with an unusually low percentage of Australian-born residents.
* The distribution of `median_income` is slightly right-skewed, but it is the closest to a normal distribution among all the variables, suggesting a relatively more balanced distribution of median income across suburbs.
"""

# Histograms
suburb_info.hist(bins=50, figsize=(20,15))
plt.show()

"""To visually explore the relationships between pairs of variables in the `suburb_info` dataset, we use a matrix of scatter plots. Additionally, to quantify the relationships, we compute the correlation matrix for the numeric columns and visualize it using a heatmap.

Based on the plots below, both the scatter plots and correlation matrix portray that there is a strong positive correlation (0.72) between `median_house_price` and `median_income`. This suggests that suburbs with higher median incomes tend to have higher median house prices.

Meanwhile, both `number_of_houses` and `population` exhibit a negative relationship with `median_house_price`. Specifically, `number_of_houses` has a weak negative correlation (-0.10), while `population` has a more pronounced negative correlation (-0.29), indicating that suburbs with more houses or larger populations tend to have lower median house prices.

Despite the right-skewed distribution of `number_of_units`, there is a positive correlation (0.34) with `median_house_price`. This relationship is not immediately obvious in the scatter plots due to the skewness of the data, but the correlation matrix confirms this positive association.

Additionally, the scatter plot for `aus_born_perc` and `median_house_price` shows a positive correlation, which is confirmed by the correlation matrix (0.30). This suggests that suburbs with higher percentages of Australian-born residents tend to have higher median house prices.

These provide valuable insights into the relationships between variables, highlighting how certain factors such as median income and percentage of Australian-born population in the property suburb positively influence median house prices, while the number of houses and population show negative associations. Understanding these correlations is crucial for developing an effective linear regression model to predict `median_house_price`.
"""

# Scatter plots
sns.pairplot(suburb_info)

# Correlation matrix for numeric columns only
numeric_cols = suburb_info.select_dtypes(include=[np.number])
corr_matrix = numeric_cols.corr()
sns.heatmap(corr_matrix, annot=True)

"""To further explore and understand the characteristics and relationships within the `suburb_info` dataset, we generate a comprehensive profiling report which provides detailed insights.

This includes:
* **Descriptive Statistics:** Summary statistics for each variable, such as mean, median, standard deviation, minimum, and maximum values, as well as percentiles. This helps in understanding the central tendency, dispersion, and distribution of the data.
* **Distribution of Variables:** Histograms and distribution plots for each variable to visualize their distribution, skewness, and potential outliers. This assists in identifying variables that may require transformation or normalization.
* **Interactions:** Scatter plots and pair plots that show the relationships between pairs of variables, providing a visual representation of how one variable may affect another. This is particularly useful for spotting trends, clusters, and potential predictors for the regression model.
* **Correlation Analysis:** A correlation matrix that highlights the relationships between different numerical variables. This is crucial for identifying potential multicollinearity issues and understanding how variables influence each other, particularly the target variable `median_house_price`.
* **Missing Values:** Analysis of missing values across the dataset, though in this case, we already confirmed that there are no missing values.
"""

# EDA for Data
from ydata_profiling import ProfileReport
profile_suburb = ProfileReport(suburb_info, title="Profiling Report")
profile_suburb

"""<div class="alert alert-block alert-info">
    
### **6.2. Scaling Methods** <a class="anchor" name="scaling_methods"></a>

When preparing data for a linear regression model, it is crucial to ensure that all features are on the same scale. This standardization allows the model to process the data more effectively, as features with larger scales can disproportionately influence the results.

In this section, we explore four different scaling methods to achieve this uniformity:

* **Z-Score Normalization (Standardization):** This method transforms the data such that each feature has a mean of 0 and a standard deviation of 1. It is particularly useful when the data follows a Gaussian distribution, allowing the model to treat all features equally.
* **Log Scaling:** This is applied to reduce skewness in the data, particularly for features with highly skewed distributions. By taking the logarithm of the data, we can compress the range of large values and expand the range of small values, making the distribution more symmetrical and helping to stabilize the variance.
* **Min-Max Scaling:** This transforms the data to fit within a specific range, usually between 0 and 1. This method preserves the relationships between data points and is useful when the data does not contain outliers, as it is sensitive to extreme values.
* **Robust Scaling:** This method uses the median and the interquartile range to scale the data (i.e. removing the median and scaling data according to the quantile range), making it less sensitive to outliers compared to other methods. This is particularly advantageous when the dataset contains significant outliers, ensuring that the scaled data remains robust and reliable.

<div class="alert alert-block alert-info">
    
#### **6.2.1. Z-Score Normalization (Standardization)** <a class="anchor" name="z_score"></a>

Firstly, the initial plot shows the distribution of numeric variables in the `suburb_info` dataset before any scaling is applied. The variables are on different scales, making it difficult to compare their effects and relationships directly.

For instance, `median_house_price` has values ranging from around 500,000 to almost 3 million, which are significantly higher compared to other variables, thus making it difficult to discern the patterns in other variables. These other variables such as `number_of_houses`, `number_of_units`, `aus_born_perc`, `median_income`, and `population` are compressed near the x-axis, making their variation hard to visualize.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

# Select numeric columns
numeric_cols = suburb_info_1.select_dtypes(include=[np.number])

# Initialize scaler
scaler = StandardScaler()

# Apply scaler
scaled_data = scaler.fit_transform(numeric_cols)
scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols.columns)

# Replace original columns with scaled ones in the copy
suburb_info_1[numeric_cols.columns] = scaled_df

"""After applying Z-score normalization, the plot depicts the distributions of the same variables but on a standardized scale, with all variables now having a mean of 0 and a standard deviation of 1. This standardization makes them directly comparable,  as the variability in each variable appears to be similar and no variable dominates the scale. Hence, it can be concluded that Z-score normalization has successfully scaled all variables to a common scale, as evidenced by the overlapping range of values on the y-axis, which now ranges from approximately -2 to 6 for all variables.

However, it's essential to acknowledge the limitations of Z-score normalization, particularly in the context of our data distribution. Our data is not normally distributed, as indicated by the high right-skewness in variables such as `number_of_houses` and `number_of_units`, as well as the moderately left-skewed distribution of `aus_born_perc`.

While Z-score normalization is beneficial in algorithms assuming normally distributed data, its application to skewed distributions may lead to distorted interpretations. In such cases, alternative scaling methods tailored to our data distribution, such as robust scaling for handling outliers or logarithmic transformation for highly skewed data, should be considered to ensure more accurate analysis and modeling outcomes.

It's also important to note that while Z-score normalization is widely applicable, it may not always be the best choice, especially if the data distribution significantly deviates from normality or if there are outliers present, as is the case with variables in our suburb data.
"""

# Plot before scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info[column].plot(label=column)
plt.title('Before Scaling')
plt.legend()
plt.show()

# Plot after scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info_1[column].plot(label=column)
plt.title('After Z-Score Normalisation')
plt.legend()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **6.2.2. Log Scaling** <a class="anchor" name="log_scaling"></a>

After applying log scaling, the subsequent plot displays the distributions of the same variables transformed using the natural logarithm.

The log transformation compresses the range of the `median_house_price`, bringing it closer in scale to the other variables. This transformation is particularly effective in reducing the impact of extreme values and skewness in the data, resulting in a more symmetric distribution that highlights underlying patterns not visible before. As a result, each variable is now more evenly spread across the plot, providing a clearer comparison of their relationships and variations.

However, despite the improvements, some disparities remain apparent. There is still a noticeable gap between `median_house_price` and the other variables, with `median_house_price` positioned at the very top of the plot. Additionally, the gaps between the other variables and each other are still considerably large compared to what was previously observed with Z-score normalization. For example, `aus_born_perc` remains on a relatively lower scale whereas `population` is on a considerably higher scale compared to the other variables.

This suggests that while log scaling helps align the distributions of variables to some extent, it may not completely eliminate the disparities in scale observed among the variables. Log scaling may not evenly compress the range of values across all parts of the distribution. In cases where the data has a wide range of values or extreme outliers, the compression effect may not be uniform, leading to distortion in the transformed distribution. This is particularly relevant to our suburb data, which contains variables with high right-skewness and outliers, potentially impacting the uniformity of compression.

Additionally, log scaling reduces the impact of extreme values, but it may not completely eliminate them. Extreme values can still have a disproportionate influence on the transformed data, particularly if they are present in large numbers or if they are outliers. Given that our suburb data includes variables with extreme values and outliers, such as in `median_house_price` and `population`, log scaling may not fully address their influence on the transformed distributions.

Therefore, further exploration and consideration of alternative scaling methods may be warranted to achieve a more balanced representation of the data.
"""

# Create a copy of the DataFrame
suburb_info_2 = suburb_info.copy()

# Select numeric columns
numeric_cols = suburb_info_2.select_dtypes(include=[np.number])

# Apply log scaling
for column in numeric_cols.columns:
    suburb_info_2[column] = np.log(numeric_cols[column])

# Plot before scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info[column].plot(label=column)
plt.title('Before Scaling')
plt.legend()
plt.show()

# Plot after scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info_2[column].plot(label=column)
plt.title('After Log Scaling')
plt.legend()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **6.2.3. Min-Max Scaling** <a class="anchor" name="min_max"></a>

Min-Max scaling is a technique used to rescale features to a range of [0, 1]. This is achieved by subtracting the minimum value of each feature and dividing by the range (max - min) of the feature. After scaling, all variables now lie within the range [0, 1], allowing for direct comparison on the same plot as displayed below.

While this equalizes the scale of the variables without distorting their distribution, it's important to recognize Min-Max scaling's sensitivity to outliers. The presence of outliers can significantly impact the range and distribution of the scaled data.

Considering the nature of our suburb data, where significant outliers exist in most of our variables of interest, Min-Max scaling may not be suitable. Its sensitivity to outliers could distort the scaled data and affect the reliability of subsequent analysis and modeling efforts.

Therefore, while Min-Max scaling can be effective in certain scenarios, its limitations and sensitivity to outliers make it less suitable for our suburb data. Alternative scaling methods that are more robust to outliers may be more appropriate in this context.
"""

# Create a copy of the DataFrame
suburb_info_3 = suburb_info.copy()

# Select numeric columns
numeric_cols = suburb_info_3.select_dtypes(include=[np.number])

# Normalize selected columns and store the scaled values in new columns
scaler = MinMaxScaler()

# Apply scaler and store the scaled values in new columns
for column in numeric_cols.columns:
    suburb_info_3[f'{column}_scaled'] = scaler.fit_transform(suburb_info_3[[column]])

# Plot before scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info[column].plot(label=column)
plt.title('Before Scaling')
plt.legend()
plt.show()

# Plot after scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info_3[f'{column}_scaled'].plot(label=f'{column}_scaled')
plt.title('After Min-Max Scaling')
plt.legend()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **6.2.4. Robust Scaling** <a class="anchor" name="robust"></a>

The last method we will try is robust scaling, offering a solution for dealing with outliers by scaling data based on percentiles rather than mean and standard deviation.

The effectiveness of robust scaling is evident in the second plot, where all numeric columns now exhibit similar ranges and distributions. This transformation brings all numeric features to a comparable scale, facilitating better interpretation and analysis of the data, particularly in scenarios where features have vastly different scales or contain outliers.

In contrast to standard scaling methods such as Z-score normalization, robust scaling is less influenced by outliers. This robustness ensures that extreme values do not disproportionately impact the scaled data, resulting in a more reliable representation of the underlying distribution.

After robust scaling, all numeric variables appear to be centered around zero, as depicted in the plot where the lines representing each variable cluster closely around the zero point on the y-axis. Additionally, the reduction in variation between variables suggests that the scaling process has effectively minimized the impact of outliers and discrepancies in the data distribution.

Although some variables, such as `number_of_units`, still exhibit noticeable peaks and valleys, these variations are now on the same scale, enabling a clearer interpretation of the data.

Overall, robust scaling has successfully standardized the data, making it more suitable for analysis and modeling purposes, particularly in the presence of outliers or varying scales among features.
"""

# Create a copy of the DataFrame
suburb_info_4 = suburb_info.copy()

# Select numeric columns
numeric_cols = suburb_info_4.select_dtypes(include=[np.number])

# Initialize a RobustScaler
scaler = RobustScaler()

# Apply robust scaling
for column in numeric_cols.columns:
    suburb_info_4[column] = scaler.fit_transform(suburb_info_4[[column]])

# Plot before scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info[column].plot(label=column)
plt.title('Before Scaling')
plt.legend()
plt.show()

# Plot after scaling
plt.figure(figsize=(12, 6))
for column in numeric_cols.columns:
    suburb_info_4[f'{column}'].plot(label=f'{column}_scaled')
plt.title('After Robust Scaling')
plt.legend()
plt.show()

"""Based on our analysis above, robust scaling is chosen as the preferred scaling method for our suburb data due to its ability to handle outliers more effectively compared to other methods, particularly in variables such as `number_of_houses` and `number_of_units`, which exhibit significant right-skewness.

By scaling the data based on percentiles rather than mean and standard deviation, robust scaling minimizes the influence of extreme values while ensuring that all numeric columns are on a comparable scale. This normalization facilitates better analysis and interpretation of the data, especially when dealing with features of varying scales or containing outliers.

Additionally, robust scaling reduces sensitivity to extreme values, making it advantageous in datasets where outliers are prevalent. Overall, robust scaling emerges as a reliable method for our suburb data, offering effective handling of outliers while maintaining the integrity of the data distribution and scale.
"""

# Select numeric columns
numeric_cols = suburb_info.select_dtypes(include=[np.number])

# Initialize RobustScaler
scaler = RobustScaler()

# Apply scaler
scaled_data = scaler.fit_transform(numeric_cols)
scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols.columns)

# Replace original columns with scaled ones in the copy
suburb_info[numeric_cols.columns] = scaled_df

"""Based on the histograms of the scaled columns in the suburb data, we can make the following observations:

* **Number of Houses and Number of Units:** These variables remain right-skewed, indicating that the majority of suburbs still have relatively low numbers of houses and units. However, the outliers are not as extreme as before, suggesting that robust scaling has successfully mitigated their impact. This shows that the scaling process has reduced the influence of extreme values while preserving the overall distribution shape.

* **Median House Price and Population:** Similar to the previous variables, both median house price and population continue to exhibit moderate right-skewness. The extreme values are less pronounced, which indicates that the scaling process has effectively reduced the influence of outliers, resulting in more symmetric and balanced distributions.

* **Australian-Born Percentage (aus_born_perc):** This variable remains left-skewed, showing that a majority of suburbs have higher percentages of Australian-born residents. Importantly, there is no longer a distinct outlier on the left side of the distribution, indicating that robust scaling has addressed this issue effectively.

Overall, robust scaling has significantly improved the distributions of the variables by reducing the impact of extreme outliers. This results in more balanced and interpretable data distributions, facilitating better analysis and comparison across different features.
"""

columns_to_scale = ['number_of_houses', 'number_of_units', 'population', 'aus_born_perc', 'median_income', 'median_house_price']

plt.figure(figsize=(12, 8))
for i, column in enumerate(columns_to_scale, 1):
    plt.subplot(2, 3, i)
    suburb_info[column].hist()
    plt.title(column)

"""<div class="alert alert-block alert-info">
    
### **6.3. Transformation Methods (Visualizations)** <a class="anchor" name="transform_methods"></a>

In this section, we will explore the effect of different transformation methods on our predictors: `number_of_houses`, `number_of_units`, `aus_born_perc`, `population` and `median_income`. By applying various transformation techniques, we aim to understand how each method impacts the distribution and comparability of these variables.

This exploration will help us identify the most suitable transformation method for our variables, ensuring that the data is well-prepared for a linear regression model to predict `median_house_price`.

Based on the scatterplot below, `median_income` does not seem to require any transformation as it appears to have a linear relationship with `median_house_price`. This means that we can directly use the median income values to analyze their relationship with median house prices.

In general, as median income increases, the median house price also tends to increase.
"""

plt.figure(figsize=(10, 6))
plt.scatter(suburb_info['median_income'], suburb_info['median_house_price'])
plt.xlabel('Median Income')
plt.ylabel('Median House Price')
plt.title('Median House Price vs Median Income')
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **6.3.1. number_of_houses & number_of_units** <a class="anchor" name="houses_units"></a>

Given that `number_of_houses` and `number_of_units` have similar distributions, both being highly right-skewed, we will compare the effects of different transformation methods on these variables side-by-side.

<div class="alert alert-block alert-info">
    
##### **Log Transformation** <a class="anchor" name="log_transform_houses"></a>

Based on the plot, we can see that the data for `number_of_houses` and `number_of_units` have been log-transformed and then plotted against `median_house_price`. Log transformation is often used to deal with skewed data and bring the distribution closer to normal.

Based on the histograms of the log-transformed features, we can observe that the transformed `number_of_houses` data is now more symmetric and has a narrower spread compared to previously. However, the distribution for `number_of_units` still appears to be right-skewed.

Upon analyzing the scatterplots, we can observe that the data points are widely dispersed for both variables, with no clear, strong correlation between the variables and `median_house_price`. The scatterplots do not show a perfect linear relationship, as there are some outliers and variations in the data points. Surprisingly, the scatterplot for `number_of_units` shows a slightly more pronounced positive correlation compared to the scatterplot for `number_of_houses`, despite its right-skewed distribution. This suggests that the number of units in a suburb may have a stronger influence on the median house price compared to the number of houses.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

dependent_variable = 'median_house_price'

# Applying log transformation to the specified features
features_to_log_transform = ['number_of_houses', 'number_of_units']
suburb_info_1[features_to_log_transform] = suburb_info_1[features_to_log_transform].apply(np.log1p)

# Plotting histograms for log-transformed features to visualize the transformation
plt.figure(figsize=(10, 4))
for i, column in enumerate(features_to_log_transform, 1):
    plt.subplot(1, len(features_to_log_transform), i)
    suburb_info_1[column].hist(alpha = 0.5, color='g')
    plt.title(f'{column} (Log Transformed)')

plt.tight_layout()
plt.show()

# Plotting scatter plots for log-transformed features against the dependent variable
plt.figure(figsize=(10, 4))
for i, column in enumerate(features_to_log_transform, 1):
    plt.subplot(1, len(features_to_log_transform), i)
    plt.scatter(suburb_info_1[column], suburb_info_1[dependent_variable], alpha=0.5)
    plt.title(f'{column} (Log Transformed) vs. Median House Price')
    plt.xlabel(f'{column} (Log Transformed)')
    plt.ylabel('Median House Price')

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Root Transformations** <a class="anchor" name="root_transform_houses"></a>

For `number_of_houses`, the histogram shows that, apart from some outliers on the right side, the distribution after square root transformation appears almost normal. This suggests that the square root transformation effectively mitigates the right-skewness of the original data. However, for cube root transformation, the histogram appears almost bimodal, indicating that the cube root transformation introduces a noticeable change in the `number_of_houses` distribution shape.

Meanwhile, for `number_of_units`, the right-skewness is still apparent even after square root transformation with noticeable outliers, indicating that the transformation has not really mitigated the skewness. This right-skewness is more apparent after applying cube root transformation, and outliers remain prominent, suggesting the transformation does not fully address the distribution's skewness.

It appears that even after applying both square root and cube root transformations to the variables `number_of_houses` and `number_of_units`, the relationship with `median_house_price` remains largely nonlinear. The scatterplots demonstrate wide dispersion of data points, suggesting that neither transformation significantly enhances the linearity between the independent variables and the dependent variable.

Interestingly, when considering the square root transformation specifically for `number_of_units`, there seems to be a very slight positive correlation with `median_house_price`. However, this correlation is not strong enough to establish a clear linear relationship, as indicated by the scattered distribution of data points. Additionally, the cube root transformation seems to result in two distinct clusters for both `number_of_houses` and `number_of_units`, further indicating the complexity of the relationship between these variables.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

dependent_variable = 'median_house_price'

# Applying square root and cube root transformations to the specified features
features_to_transform = ['number_of_houses', 'number_of_units']
for feature in features_to_transform:
    suburb_info_1[f'{feature}_sr'] = np.sqrt(suburb_info_1[feature])
    suburb_info_1[f'{feature}_cr'] = np.cbrt(suburb_info_1[feature])

# Plotting histograms for square root and cube root transformed features
plt.figure(figsize=(12, 6))
transformations = ['sr', 'cr']  # sr: Square Root, cr: Cube Root
transformation_names = ['Square Root', 'Cube Root']
for i, feature in enumerate(features_to_transform, 1):
    for j, transformation in enumerate(transformations, 1):
        plt.subplot(len(features_to_transform), len(transformations), (i-1)*len(transformations)+j)
        plt.hist(suburb_info_1[f'{feature}_{transformation}'], bins=30, alpha=0.5, color='g')
        plt.title(f'{feature} ({transformation_names[j-1]} Transformed)')
        plt.xlabel(f'{feature} ({transformation_names[j-1]} Transformed)')
        plt.ylabel('Frequency')

plt.tight_layout(pad=2.0)
plt.show()

# Plotting scatter plots for square root and cube root transformed features against the dependent variable
plt.figure(figsize=(12, 6))
for i, feature in enumerate(features_to_transform, 1):
    for j, transformation in enumerate(transformations, 1):
        plt.subplot(len(features_to_transform), len(transformations), (i-1)*len(transformations)+j)
        plt.scatter(suburb_info_1[f'{feature}_{transformation}'], suburb_info_1[dependent_variable], alpha=0.5)
        plt.title(f'{feature} ({transformation_names[j-1]} Transformed) vs. Median House Price')
        plt.xlabel(f'{feature} ({transformation_names[j-1]} Transformed)')
        plt.ylabel('Median House Price')

plt.tight_layout(pad=2.0)
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Power Transformations** <a class="anchor" name="power_transform_houses"></a>

For both `number_of_houses` and `number_of_units`, the square, cube and fourth power transformations significantly distort the distributions, resulting in histograms with only one prominent mode. This indicates a loss of meaningful distributional information.

When looking at the histograms after reciprocal transformation, the `number_of_houses` histogram remains right-skewed with pronounced outliers on the right side. Conversely, the right-skewness for `number_of_units` is reduced compared to the original data, resulting in a moderately right-skewed histogram.

As for inverse root transformations, the histogram for `number_of_houses` shows less right-skewness compared to the reciprocal transformation, though it remains moderately right-skewed. Meanwhile, the distribution for `number_of_units` appears more normal, suggesting a better handling of the right-skewness in the original data.

In examining the scatterplots resulting from square, cube, and fourth power transformations, it is evident that data points for both `number_of_houses` and `number_of_units` tend to cluster on the left side, indicating a weak association with `median_house_price`. Conversely, when employing reciprocal and inverse root transformations for both variables, the scatterplots display a dispersion of data points without any discernible correlation with `median_house_price`. This lack of pattern suggests that these transformations do not enhance the linearity or predictability of the variables concerning the dependent variable.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

dependent_variable = 'median_house_price'

powers = [-1, -0.5, 2, 3, 4]  # reciprocal, inverse root, square, cube, and fourth power
power_names = ['Reciprocal', 'Inverse Root', 'Square', 'Cube', 'Fourth Power']  # corresponding names

# Apply power transformations
features_to_transform = ['number_of_houses', 'number_of_units']
for feature in features_to_transform:
    for power in powers:
        suburb_info_1[f'{feature}_p{power}'] = np.power(suburb_info_1[feature] + 1, power)

# Plotting histograms for power transformed features
plt.figure(figsize=(10, 25))
i = 1
for power, power_name in zip(powers, power_names):
    for feature in features_to_transform:
        plt.subplot(len(powers), len(features_to_transform), i)
        plt.hist(suburb_info_1[f'{feature}_p{power}'], bins=30, alpha=0.5, color='g')
        plt.title(f'{feature} ({power_name} Transformed)')
        plt.xlabel(f'{feature} ({power_name} Transformed)')
        plt.ylabel('Frequency')
        i += 1

plt.tight_layout()
plt.show()

# Plotting scatter plots of power transformed features against median_house_price
plt.figure(figsize=(10, 25))
i = 1
for power, power_name in zip(powers, power_names):
    for feature in features_to_transform:
        plt.subplot(len(powers), len(features_to_transform), i)
        plt.scatter(suburb_info_1[f'{feature}_p{power}'], suburb_info_1[dependent_variable], alpha=0.5)
        plt.title(f'{feature} ({power_name}) vs. Median House Price')
        plt.xlabel(f'{feature} ({power_name})')
        plt.ylabel('Median House Price')
        i += 1

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Box-Cox & Quantile Transformations** <a class="anchor" name="boxcox_transform_houses"></a>

The Box-Cox transformation appears to have almost normalized the distribution for `number_of_houses`, while `number_of_units` still exhibits a slight right-skew.

However, the Quantile transformation successfully achieved perfectly normal distributions for both variables, albeit with outliers on each extreme.

In terms of scatterplots, the Box-Cox transformation shows a dispersed pattern for `number_of_houses` with no clear correlation, while for `number_of_units`, it displays a somewhat positive correlation, although not perfect. On the other hand, the Quantile scatterplots reveal a clustering around the center, with a slight upward spread, suggesting a very weak positive correlation for both `number_of_houses` and `number_of_units`.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

# List of features to transform
features_to_transform = ['number_of_houses', 'number_of_units']

# Initialize QuantileTransformer
qt = QuantileTransformer(output_distribution='normal', n_quantiles=500, random_state=2)

for feature in features_to_transform:
    # Shift data to be positive
    min_value = suburb_info_1[feature].min()
    shift = 1.01 - min_value  # shift to make all data slightly above zero
    suburb_info_1[f'{feature}_shifted'] = suburb_info_1[feature] + shift

    # Apply Box-Cox transformation
    suburb_info_1[f'{feature}_boxcox'], _ = boxcox(suburb_info_1[f'{feature}_shifted'])

    # Apply Quantile transformation
    suburb_info_1[f'{feature}_quantile'] = qt.fit_transform(suburb_info_1[[feature]])

# Plotting histograms for transformed features
plt.figure(figsize=(12, 12))

for i, feature in enumerate(features_to_transform, 1):
    # Box-Cox
    plt.subplot(len(features_to_transform), 2, 2*i-1)
    plt.hist(suburb_info_1[f'{feature}_boxcox'], bins=30, alpha=0.5, color='g')
    plt.title(f'{feature} (Box-Cox Transformed)')
    plt.xlabel(f'{feature} (Box-Cox Transformed)')
    plt.ylabel('Frequency')

    # Quantile
    plt.subplot(len(features_to_transform), 2, 2*i)
    plt.hist(suburb_info_1[f'{feature}_quantile'], bins=30, alpha=0.5, color='g')
    plt.title(f'{feature} (Quantile Transformed)')
    plt.xlabel(f'{feature} (Quantile Transformed)')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Plotting scatter plots of transformed features against median_house_price
plt.figure(figsize=(12, 12))

for i, feature in enumerate(features_to_transform, 1):
    # Box-Cox
    plt.subplot(len(features_to_transform), 2, 2*i-1)
    plt.scatter(suburb_info_1[f'{feature}_boxcox'], suburb_info_1['median_house_price'], alpha=0.5)
    plt.title(f'{feature} (Box-Cox) vs. Median House Price')
    plt.xlabel(f'{feature} (Box-Cox)')
    plt.ylabel('Median House Price')

    # Quantile
    plt.subplot(len(features_to_transform), 2, 2*i)
    plt.scatter(suburb_info_1[f'{feature}_quantile'], suburb_info_1['median_house_price'], alpha=0.5)
    plt.title(f'{feature} (Quantile) vs. Median House Price')
    plt.xlabel(f'{feature} (Quantile)')
    plt.ylabel('Median House Price')

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **6.3.2. aus_born_perc** <a class="anchor" name="aus_born_perc"></a>

<div class="alert alert-block alert-info">
    
##### **Log Transformation** <a class="anchor" name="log_transform_ausborn"></a>

When applying log transformation to `aus_born_perc`, the histogram maintains the left-skewed distribution of the original data, indicating that the transformation preserved the essence of the initial distribution.

In terms of the scatterplot depicting the relationship between the log-transformed `aus_born_perc` and `median_house_price`, a positive correlation is observed. However, the majority of data points appear clustered on the right side, suggesting that while there is a positive association, there may be limitations in capturing the full range of variability within the data.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

dependent_variable = 'median_house_price'

# Applying log transformation to the specified features
features_to_log_transform = ['aus_born_perc']
suburb_info_1[features_to_log_transform] = suburb_info_1[features_to_log_transform].apply(np.log1p)

# Plotting histograms for log-transformed feature to visualize the transformation
plt.figure(figsize=(12, 4))
for i, column in enumerate(features_to_log_transform, 1):
    plt.subplot(1, 3, i)
    suburb_info_1[column].hist(alpha = 0.5, color='g')
    plt.title(f'{column} (Log Transformed)')

plt.tight_layout()
plt.show()

# Plotting scatter plots for log-transformed feature against the dependent variable
plt.figure(figsize=(12, 4))
for i, column in enumerate(features_to_log_transform, 1):
    plt.subplot(1, 3, i)
    plt.scatter(suburb_info_1[column], suburb_info_1[dependent_variable], alpha=0.5)
    plt.title(f'{column} (Log Transformed) vs. Median House Price')
    plt.xlabel(f'{column} (Log Transformed)')
    plt.ylabel('Median House Price')

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Root Transformations** <a class="anchor" name="root_transform_ausborn"></a>

The cube root transformation applied to `aus_born_perc` resulted in a bimodal histogram, suggesting a more complex distribution. Correspondingly, the scatterplot exhibits two distinct clusters, one on the right and another on the left side, indicating potential segmentation within the data.

On the other hand, the square root transformation produced a histogram that appears nearly normal, although there's a notable outlier on the left side. However, the scatterplot does not reveal any clear correlation between the square root-transformed `aus_born_perc` and `median_house_price`, implying that this transformation may not effectively capture the relationship between the variables.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

dependent_variable = 'median_house_price'

# Applying the cube root and square root transformations
suburb_info_1['aus_born_perc_cr'] = np.cbrt(suburb_info_1['aus_born_perc'])
suburb_info_1['aus_born_perc_sr'] = np.sqrt(suburb_info_1['aus_born_perc'])

# Plotting histograms for cube root and square root transformed features
plt.figure(figsize=(15, 5))
transformations = ['cr', 'sr']  # cr: Cube Root, sr: Square Root
transformation_names = ['Cube Root', 'Square Root']
i = 1
for transformation, transformation_name in zip(transformations, transformation_names):
    plt.subplot(2, 2, i)
    plt.hist(suburb_info_1[f'aus_born_perc_{transformation}'], bins=30, alpha=0.5, color='g')
    plt.title(f'Aus Born Perc ({transformation_name} Transformed)')
    plt.xlabel(f'Aus Born Perc ({transformation_name} Transformed)')
    plt.ylabel('Frequency')
    i += 1

# Plotting scatter plots of cube root and square root transformed features against median_house_price
i = 1
for transformation, transformation_name in zip(transformations, transformation_names):
    plt.subplot(2, 2, i+2)
    plt.scatter(suburb_info_1[f'aus_born_perc_{transformation}'], suburb_info_1['median_house_price'], alpha=0.5)
    plt.title(f'Aus Born Perc ({transformation_name}) vs. Median House Price')
    plt.xlabel(f'Aus Born Perc ({transformation_name})')
    plt.ylabel('Median House Price')
    i += 1

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Power Transformations** <a class="anchor" name="power_transform_ausborn"></a>

The power transformations applied to `aus_born_perc` yielded histograms that predominantly exhibit right-skewed distributions across all transformations, contrary to the original left-skewed distribution of `aus_born_perc`.

Regarding the scatterplots depicting the relationship between the transformed `aus_born_perc` feature and `median_house_price`, there isn't a clear correlation observed. However, there seems to be a very subtle negative trend in the scatterplots, although not strongly pronounced.

These observations suggest that the power transformations have altered the distributional shape of the data for `aus_born_perc`, and they may not have effectively captured the underlying relationship with `median_house_price`.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

powers = [-1, -0.5, 2, 3, 4]  # reciprocal, inverse root, square, cube, and fourth power
power_names = ['Reciprocal', 'Inverse Root', 'Square', 'Cube', 'Fourth Power']  # corresponding names

# Apply power transformations
for power in powers:
    suburb_info_1[f'aus_born_perc_p{power}'] = np.power(suburb_info_1['aus_born_perc'] + 1, power)

# Plotting histograms for power transformed features
plt.figure(figsize=(18, 12))
i = 1
for power, power_name in zip(powers, power_names):
    plt.subplot(5, 2, 2*i-1)
    plt.hist(suburb_info_1[f'aus_born_perc_p{power}'], bins=30, alpha=0.5, color='g')
    plt.title(f'Aus Born Perc ({power_name} Transformed)')
    plt.xlabel(f'Aus Born Perc ({power_name} Transformed)')
    plt.ylabel('Frequency')
    i += 1

# Plotting scatter plots of power transformed features against median_house_price
i = 1
for power, power_name in zip(powers, power_names):
    plt.subplot(5, 2, 2*i)
    plt.scatter(suburb_info_1[f'aus_born_perc_p{power}'], suburb_info_1['median_house_price'], alpha=0.5)
    plt.title(f'Aus Born Perc ({power_name}) vs. Median House Price')
    plt.xlabel(f'Aus Born Perc ({power_name})')
    plt.ylabel('Median House Price')
    i += 1

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Box-Cox & Quantile Transformations** <a class="anchor" name="boxcox_transform_ausborn"></a>

The Box-Cox transformation results in a histogram that appears slightly left-skewed but generally approximates a normal distribution. Conversely, the Quantile transformation yields a histogram that closely resembles a normal distribution.

Regarding the scatterplots depicting the relationship between the transformed `aus_born_perc` features and `median_house_price`, no obvious correlation is discernible for both transformations. However, the scatterplot generated from the Quantile transformation shows a more clustered distribution of points, indicating potentially tighter grouping of data points. Notably, the scatterplot from the Box-Cox transformation appears to exhibit an almost positive correlation, but with points quite spread out.

These observations suggest that while both transformations improved the distributional characteristics of the `aus_born_perc` data, neither transformation clearly established a significant linear correlation with the dependent variable.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

# Shift data to be positive
min_value = suburb_info_1['aus_born_perc'].min()
shift = 1.01 - min_value  # shift to make all data slightly above zero
suburb_info_1['aus_born_perc_shifted'] = suburb_info_1['aus_born_perc'] + shift

# Apply Box-Cox transformation
suburb_info_1['aus_born_perc_boxcox'], _ = boxcox(suburb_info_1['aus_born_perc_shifted'])

# Initialize QuantileTransformer
qt = QuantileTransformer(output_distribution='normal', n_quantiles=500, random_state=2)

# Apply Quantile transformation
suburb_info_1['aus_born_perc_quantile'] = qt.fit_transform(suburb_info_1[['aus_born_perc']])

# Plotting histograms for transformed features
plt.figure(figsize=(12, 6))

# Box-Cox
plt.subplot(2, 2, 1)
plt.hist(suburb_info_1['aus_born_perc_boxcox'], bins=30, alpha=0.5, color='g')
plt.title('Aus Born Perc (Box-Cox Transformed)')
plt.xlabel('Aus Born Perc (Box-Cox Transformed)')
plt.ylabel('Frequency')

# Quantile
plt.subplot(2, 2, 3)
plt.hist(suburb_info_1['aus_born_perc_quantile'], bins=30, alpha=0.5, color='g')
plt.title('Aus Born Perc (Quantile Transformed)')
plt.xlabel('Aus Born Perc (Quantile Transformed)')
plt.ylabel('Frequency')

# Plotting scatter plots of transformed features against median_house_price

# Box-Cox
plt.subplot(2, 2, 2)
plt.scatter(suburb_info_1['aus_born_perc_boxcox'], suburb_info_1['median_house_price'], alpha=0.5)
plt.title('Aus Born Perc (Box-Cox) vs. Median House Price')
plt.xlabel('Aus Born Perc (Box-Cox)')
plt.ylabel('Median House Price')

# Quantile
plt.subplot(2, 2, 4)
plt.scatter(suburb_info_1['aus_born_perc_quantile'], suburb_info_1['median_house_price'], alpha=0.5)
plt.title('Aus Born Perc (Quantile) vs. Median House Price')
plt.xlabel('Aus Born Perc (Quantile)')
plt.ylabel('Median House Price')

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
#### **6.3.3. population** <a class="anchor" name="population"></a>

<div class="alert alert-block alert-info">
    
##### **Log Transformation** <a class="anchor" name="log_transform_population"></a>

The histogram of the log-transformed `population` indicates a left-skewed distribution, which contrasts with the original right-skewed distribution. This divergence suggests that the log transformation may not have effectively addressed the skewness in the data.

Additionally, the scatterplot depicting the relationship between the log-transformed `population` and `median_house_price` reveals a subtle negative correlation, implying that higher log-transformed `population` values may be associated with slightly lower median house prices. However, the strength of this relationship appears relatively weak.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

dependent_variable = 'median_house_price'

# Applying log transformation to the specified features
features_to_log_transform = ['population']
suburb_info_1[features_to_log_transform] = suburb_info_1[features_to_log_transform].apply(np.log1p)

# Plotting histograms for log-transformed feature to visualize the transformation
plt.figure(figsize=(12, 4))
for i, column in enumerate(features_to_log_transform, 1):
    plt.subplot(1, 3, i)
    suburb_info_1[column].hist(alpha = 0.5, color='g')
    plt.title(f'{column} (Log Transformed)')

plt.tight_layout()
plt.show()

# Plotting scatter plots for log-transformed feature against the dependent variable
plt.figure(figsize=(12, 4))
for i, column in enumerate(features_to_log_transform, 1):
    plt.subplot(1, 3, i)
    plt.scatter(suburb_info_1[column], suburb_info_1[dependent_variable], alpha=0.5)
    plt.title(f'{column} (Log Transformed) vs. Median House Price')
    plt.xlabel(f'{column} (Log Transformed)')
    plt.ylabel('Median House Price')

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Root Transformations** <a class="anchor" name="root_transform_population"></a>

The cube root transformation for `population` resulted in a bimodal distribution, indicating a complex data pattern. Conversely, the square root transformation produced a histogram resembling a nearly normal distribution, although with two outliers on the right side.

In terms of the `population` variable's relationship with `median_house_price`, the scatterplot generated from the cube root transformation exhibited two distinct clusters, suggesting segmentation within the data rather than a clear correlation. On the other hand, the scatterplot derived from the square root transformation displayed a notable negative correlation, approaching linearity, indicating that higher square root-transformed population values tend to be associated with lower median house prices.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

dependent_variable = 'median_house_price'

# Applying the cube root and square root transformations
suburb_info_1['population_cr'] = np.cbrt(suburb_info_1['population'])
suburb_info_1['population_sr'] = np.sqrt(suburb_info_1['population'])

# Plotting histograms for cube root and square root transformed features
plt.figure(figsize=(15, 5))
transformations = ['cr', 'sr']  # cr: Cube Root, sr: Square Root
transformation_names = ['Cube Root', 'Square Root']
i = 1
for transformation, transformation_name in zip(transformations, transformation_names):
    plt.subplot(2, 2, i)
    plt.hist(suburb_info_1[f'population_{transformation}'], bins=30, alpha=0.5, color='g')
    plt.title(f'Population ({transformation_name} Transformed)')
    plt.xlabel(f'Population ({transformation_name} Transformed)')
    plt.ylabel('Frequency')
    i += 1

# Plotting scatter plots of cube root and square root transformed features against median_house_price
i = 1
for transformation, transformation_name in zip(transformations, transformation_names):
    plt.subplot(2, 2, i+2)
    plt.scatter(suburb_info_1[f'population_{transformation}'], suburb_info_1['median_house_price'], alpha=0.5)
    plt.title(f'Population ({transformation_name}) vs. Median House Price')
    plt.xlabel(f'Population ({transformation_name})')
    plt.ylabel('Median House Price')
    i += 1

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Power Transformations** <a class="anchor" name="power_transform_population"></a>

The power transformations applied to the `population` variable exhibit diverse effects on its distribution and relationship with the `median_house_price`.

The inverse root, square, and cube power transformations maintained the right-skewed distribution of `population`, yet accentuated its skewness, making it more highly right-skewed than before. Meanwhile, both the reciprocal and fourth power transformations resulted in unimodal distributions.

In terms of its relationship with `median_house_price`, the scatterplots generated from the reciprocal, cube, and fourth power transformations displayed clustering of data points without any clear correlation. Additionally, the scatterplot derived from the inverse root transformation exhibited a positively correlated pattern but appeared clumped together, indicating a non-linear relationship. In contrast, the scatterplot from the square power transformation showed a non-linear, negative correlation.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

powers = [-1, -0.5, 2, 3, 4]  # reciprocal, inverse root, square, cube, and fourth power
power_names = ['Reciprocal', 'Inverse Root', 'Square', 'Cube', 'Fourth Power']  # corresponding names

# Apply power transformations
for power in powers:
    suburb_info_1[f'population_p{power}'] = np.power(suburb_info_1['population'] + 1, power)

# Plotting histograms for power transformed features
plt.figure(figsize=(18, 12))
i = 1
for power, power_name in zip(powers, power_names):
    plt.subplot(5, 2, 2*i-1)
    plt.hist(suburb_info_1[f'population_p{power}'], bins=30, alpha=0.5, color='g')
    plt.title(f'Population ({power_name} Transformed)')
    plt.xlabel(f'Population ({power_name} Transformed)')
    plt.ylabel('Frequency')
    i += 1

# Plotting scatter plots of power transformed features against median_house_price
i = 1
for power, power_name in zip(powers, power_names):
    plt.subplot(5, 2, 2*i)
    plt.scatter(suburb_info_1[f'population_p{power}'], suburb_info_1['median_house_price'], alpha=0.5)
    plt.title(f'Population ({power_name}) vs. Median House Price')
    plt.xlabel(f'Population ({power_name})')
    plt.ylabel('Median House Price')
    i += 1

plt.tight_layout()
plt.show()

"""<div class="alert alert-block alert-info">
    
##### **Box-Cox & Quantile Transformations** <a class="anchor" name="boxcox_transform_population"></a>

The application of both Box-Cox and Quantile transformations to the `population` variable resulted in approximately normal distributions, indicating successful normalization of the data.

Meanwhile, in the scatterplot generated from the Box-Cox transformation, there appears to be a negative correlation, suggesting that higher Box-Cox-transformed `population` values may be associated with lower median house prices. However, the strength of this relationship appears to be relatively weak and non-linear. Similarly, the scatterplot for the Quantile transformation also shows a weak negative correlation, with data points mostly clumped together, indicating a lack of clear linear relationship with the dependent variable `median_house_price`.

Overall, while both transformations effectively normalized the distribution of the data, they did not establish a clear linear relationship with the dependent variable `median_house_price`.
"""

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

# Shift data to be positive
min_value = suburb_info_1['population'].min()
shift = 1.01 - min_value  # shift to make all data slightly above zero
suburb_info_1['population_shifted'] = suburb_info_1['population'] + shift

# Apply Box-Cox transformation
suburb_info_1['population_boxcox'], _ = boxcox(suburb_info_1['population_shifted'])

# Initialize QuantileTransformer
qt = QuantileTransformer(output_distribution='normal', n_quantiles=500, random_state=2)

# Apply Quantile transformation
suburb_info_1['population_quantile'] = qt.fit_transform(suburb_info_1[['population']])

# Plotting histograms for transformed features
plt.figure(figsize=(12, 6))

# Box-Cox
plt.subplot(2, 2, 1)
plt.hist(suburb_info_1['population_boxcox'], bins=30, alpha=0.5, color='g')
plt.title('Population (Box-Cox Transformed)')
plt.xlabel('Population (Box-Cox Transformed)')
plt.ylabel('Frequency')

# Quantile
plt.subplot(2, 2, 3)
plt.hist(suburb_info_1['population_quantile'], bins=30, alpha=0.5, color='g')
plt.title('Population (Quantile Transformed)')
plt.xlabel('Population (Quantile Transformed)')
plt.ylabel('Frequency')

# Plotting scatter plots of transformed features against median_house_price

# Box-Cox
plt.subplot(2, 2, 2)
plt.scatter(suburb_info_1['population_boxcox'], suburb_info_1['median_house_price'], alpha=0.5)
plt.title('Population (Box-Cox) vs. Median House Price')
plt.xlabel('Population (Box-Cox)')
plt.ylabel('Median House Price')

# Quantile
plt.subplot(2, 2, 4)
plt.scatter(suburb_info_1['population_quantile'], suburb_info_1['median_house_price'], alpha=0.5)
plt.title('Population (Quantile) vs. Median House Price')
plt.xlabel('Population (Quantile)')
plt.ylabel('Median House Price')

plt.tight_layout()
plt.show()

"""Based on our analysis of the transformation visualization plots, square root and Box-Cox transformations appear to be appropriate choices for variables exhibiting high right-skewness, such as `number_of_houses`, `number_of_units`, and `population`. These transformations either preserve the distribution shape, as seen with the square root transformation for `number_of_houses` and `number_of_units`, or result in approximately normal distributions, as observed with the Box-Cox transformation for `population`. Despite not achieving complete linearity, these transformations demonstrate some degree of correlation when plotted against `median_house_price`, as evidenced by the negative correlation observed in the scatterplot generated from the Box-Cox transformation for `population`.

Conversely, for left-skewed variables like `aus_born_perc`, the Quantile transformation appears to be the most appropriate choice as it effectively addresses left-skewness by spreading out the data uniformly across the range, thus mitigating the effects of skewness.

However, it's essential to conduct further analysis using statistical metrics such as R-squared and RMSE rather than solely relying on visual inspections to determine the most suitable transformations. The aim of determining the most appropriate transformations for these variables is to enhance their linear relationship with the target variable (`median_house_price`), as we intend to prepare the data for a linear regression model.

<div class="alert alert-block alert-info">
    
### **6.4. Testing with Linear Regression Model** <a class="anchor" name="linear_reg"></a>

<div class="alert alert-block alert-info">
    
#### **6.4.1. Linear Regression Analysis on Suburb Data (Before Transformation)** <a class="anchor" name="before_trans"></a>

In this section, we conduct an initial assessment of the predictive power of a linear regression model applied to the suburb data before any transformations. It serves as a baseline for comparison with subsequent analyses involving transformed data.

Initially, we defined our independent variables as `number_of_houses`, `number_of_units`, `population`, `aus_born_perc`, and `median_income`, and the dependent variable (target) as `median_house_price`. We then split the data into training and testing sets using an 80-20 ratio, with 80% of the data used for training and 20% for testing. This step ensures that the model's performance can be evaluated on unseen data.

Next, we initialized a linear regression model and fitted it to the training data. This process involves adjusting the parameters of the model to minimize the difference between the predicted and actual target values in the training dataset. Once the model was trained, we used it to make predictions on the testing dataset.

To evaluate the performance of our model, we calculated two metrics: R-squared (coefficient of determination) and RMSE (Root Mean Squared Error). The R-squared value indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. In this case, we obtained an R-squared value of approximately 0.656, suggesting that around 65.6% of the variance in median house prices can be explained by the independent variables.

Additionally, we calculated the RMSE, which represents the average deviation of the predicted values from the actual values. A lower RMSE value indicates better model performance. Here, we obtained an RMSE of approximately 0.511, indicating that, on average, our model's predictions deviate from the actual median house prices by approximately 0.511 units.
"""

# Before transformation (applying on robustly scaled data)

# Define independent and dependent variables
x = suburb_info[['number_of_houses', 'number_of_units', 'population', 'aus_born_perc', 'median_income']]
y = suburb_info['median_house_price']

# Split data into training and testing sets:
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

# Initialize and fit model:
model = LinearRegression()
model.fit(x_train, y_train)

# Make predictions and evaluate your model:
y_pred = model.predict(x_test)
r2 = r2_score(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f'R-squared: {r2}')
print(f'RMSE: {rmse}')

"""We also performed a more detailed analysis of the linear regression model using the `statsmodels` library. The purpose was to gain deeper insights into the relationships between the independent variables (`number_of_houses`, `number_of_units`, `population`, `aus_born_perc`, `median_income`) and the dependent variable (`median_house_price`).

The regression results indicate an R-squared value of 0.624, suggesting that approximately 62.4% of the variance in the dependent variable is explained by the independent variables. Additionally, the adjusted R-squared value, which considers the number of predictors in the model, is 0.612. It's important to note that the OLS regression R-squared obtained in this analysis is calculated based on the training data, whereas the previous R-squared value (0.656) is computed on the test data. The R-squared value obtained from the training data measures how well the model fits the training data, while the R-squared value from the test data indicates how well the model performs on new, unseen data.

Meanwhile, the F-statistic of 51.51 with a p-value of 2.96e-31 indicates that the overall model is statistically significant. The analysis also includes metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to evaluate the model's performance on both the training and testing datasets. The training MAE and RMSE are 0.3683 and 0.4683, respectively, while the testing MAE and RMSE are 0.3892 and 0.5115, respectively. These metrics provide insights into how well the model performs in predicting median house prices, with lower values indicating better performance.

Furthermore, the residual plot and the Q-Q plot of the residuals were examined to assess the assumptions of linear regression. The residual plot shows no obvious pattern in the residuals, indicating that the linear regression assumption of constant variance (homoscedasticity) is not violated. However, there are some outliers that deviate from the 45-degree line in the Q-Q plot, particularly those located at the extreme ends, both towards the lower and upper tails, indicating that the normality assumption of residuals may be slightly violated.
"""

# Fit the model using statsmodels to get more detailed output
x_train_sm = sm.add_constant(x_train)
model_sm = sm.OLS(y_train, x_train_sm)
results = model_sm.fit()

# Print out the statistics
print(results.summary())

# Calculate metrics
y_train_pred = results.predict(x_train_sm)
y_test_pred = results.predict(sm.add_constant(x_test))

print('Train Mean Absolute Error:', mean_absolute_error(y_train, y_train_pred))
print('Train Root Mean Squared Error:', np.sqrt(mean_squared_error(y_train, y_train_pred)))
print('Test Mean Absolute Error:', mean_absolute_error(y_test, y_test_pred))
print('Test Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_test_pred)))

# Residuals
residuals = y_train - y_train_pred

# Residual plot
sns.scatterplot(x=y_train_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.show()

# Normality of residuals
qqplot(residuals, line='s')
plt.show()

# Variance Inflation Factor
vif = pd.DataFrame()
vif["variables"] = x_train.columns
vif["VIF"] = [variance_inflation_factor(x_train.values, i) for i in range(x_train.shape[1])]
print(vif)

# Cross-validation
cross_val_scores = cross_val_score(LinearRegression(), x_train, y_train, cv=5)
print('Cross-validated scores:', cross_val_scores)

# Confidence intervals
print('Confidence intervals:')
print(results.conf_int())

"""<br>

Given the initial right-skewed distributions of the `number_of_houses`, `number_of_units`, and `population` variables, our focus turns to transformation methods such as square root, cube root, log, and Box-Cox. These techniques are suitable for addressing right-skewed data distributions by normalizing their shape. Right-skewed distributions are characterized by a long right tail, and these transformations compress larger values more than smaller ones, effectively reducing the skewness and bringing the distribution closer to symmetry. Moreover, they stabilize the variance across the data range, making it more homogeneous. By achieving a more linear relationship between the independent and dependent variables, these transformations align with the assumptions of linear regression modeling, enhancing the suitability of the data for such analyses.

Meanwhile, as `aus_born_perc` originally showed a left-skewed distribution, the most appropriate transformations would involve techniques such as square, cube, fourth power, inverse root, and Quantile transformation. These transformations address left-skewness by expanding smaller values more than larger ones, reducing the skewness and bringing the distribution closer to normality. The Quantile transformation specifically spreads out the data uniformly across the range, effectively mitigating the effects of skewness and ensuring a more symmetrical distribution. By normalizing the distribution and stabilizing the variance, these transformations make the `aus_born_perc` data more suitable for linear regression modeling.

<div class="alert alert-block alert-info">
    
#### **6.4.2. Transformations on number_of_houses** <a class="anchor" name="trans_houses"></a>

In this analysis, we are testing various transformations of the `number_of_houses` variable to assess their impact on the performance of a linear regression model predicting `median_house_price`. The transformations include reciprocal, inverse root, square, cube, and fourth power transformations, as well as logarithmic, square root, cube root, Box-Cox, and Quantile transformations.

We first apply these transformations to the `number_of_houses` variable and then fit a linear regression model using each transformed variable separately. The model is trained on a subset of the data and evaluated on a holdout test set. For each transformation, we calculate the R-squared and root mean squared error (RMSE) to measure the model's performance.

The results indicate that among all the transformations appropriate for right-skewed data (i.e. log, square root, cube root Box-Cox), the square root transformation yields the highest R-squared value (0.646118) and the lowest RMSE (0.519052). This suggests that the square root transformation may be the most appropriate for the `number_of_houses` variable in terms of improving the model's predictive performance for `median_house_price`.
"""

# Testing on number_of_houses:

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

powers = [-1, -0.5, 2, 3, 4]  # reciprocal, inverse root, square, cube, and fourth power

# Add a constant to the data
constant = 1 - suburb_info_1['number_of_houses'].min()
suburb_info_1['number_of_houses'] = suburb_info_1['number_of_houses'] + constant

# Apply power transformations
for power in powers:
    suburb_info_1[f'number_of_houses_p{power}'] = np.power(suburb_info_1['number_of_houses'], power)

# Apply other transformations
suburb_info_1['number_of_houses_log'] = np.log(suburb_info_1['number_of_houses'])
suburb_info_1['number_of_houses_sqrt'] = np.sqrt(suburb_info_1['number_of_houses'])
suburb_info_1['number_of_houses_cbrt'] = np.cbrt(suburb_info_1['number_of_houses'])
suburb_info_1['number_of_houses_boxcox'], _ = stats.boxcox(suburb_info_1['number_of_houses'])
qt = QuantileTransformer(output_distribution='normal')
suburb_info_1['number_of_houses_quantile'] = qt.fit_transform(suburb_info_1[['number_of_houses']])

# Define independent and dependent variables
x = suburb_info_1[['number_of_units', 'aus_born_perc', 'population', 'median_income']]
y = suburb_info_1['median_house_price']

# Initialize and fit model
model = LinearRegression()

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

# Initialize an empty list to store the results
results = []

# Test model for each transformation
transformations = [f'number_of_houses_p{power}' for power in powers] + ['number_of_houses_log', 'number_of_houses_sqrt', 'number_of_houses_cbrt', 'number_of_houses_boxcox', 'number_of_houses_quantile']
for trans in transformations:
    x_train_transformed = x_train.copy()
    x_test_transformed = x_test.copy()

    x_train_transformed['number_of_houses'] = suburb_info_1.loc[x_train.index, trans]
    x_test_transformed['number_of_houses'] = suburb_info_1.loc[x_test.index, trans]

    model.fit(x_train_transformed, y_train)

    y_pred = model.predict(x_test_transformed)
    r2 = r2_score(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    # Append the results to the list
    results.append([trans, r2, rmse])

# Convert the results list to a DataFrame
results_df = pd.DataFrame(results, columns=['Transformation', 'R-squared', 'Root Mean Squared Error'])

# Display the results
print(results_df)

"""<div class="alert alert-block alert-info">
    
#### **6.4.3. Transformations on number_of_units** <a class="anchor" name="trans_units"></a>

Among the transformations appropriate for right-skewed data, the Box-Cox transformation yielded the highest R-squared value (0.720359) and the lowest RMSE (0.461404). This indicates that the Box-Cox transformation is the most effective in normalizing the `number_of_units` variable and improving the model's predictive performance. Transformations such as the square root, cube root, and log transformations showed relatively lower R-squared values and higher RMSEs compared to Box-Cox.

Despite showing a high R-squared value and low RMSE, reciprocal transformation (`number_of_units_p-1`) is typically more suited for left-skewed data. This is because the reciprocal transformation tends to stretch out the lower end of the distribution while compressing the higher end. For right-skewed data, which have a long right tail, the goal is to compress the high values and expand the low values to reduce skewness and achieve a more symmetrical distribution.

In the context of right-skewed data, using a reciprocal transformation can exacerbate the skewness instead of reducing it. The transformation inverts the values, leading to disproportionate scaling where large values become very small and small values become very large. This can distort the relationships between the variables rather than linearizing them, which is essential for accurate linear regression modeling. Therefore, its use in this context might not be appropriate as it may not properly address the right skewness in the `number_of_units` distribution.

Therefore, for the `number_of_units` variable, which originally exhibited a right-skewed distribution, the Box-Cox transformation proves to be the most appropriate method, significantly improving the linear regression model's performance.
"""

# Testing on number_of_units:

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

powers = [-1, -0.5, 2, 3, 4]  # reciprocal, inverse root, square, cube, and fourth power

# Add a constant to the data
constant = 1 - suburb_info_1['number_of_units'].min()
suburb_info_1['number_of_units'] = suburb_info_1['number_of_units'] + constant

# Apply power transformations
for power in powers:
    suburb_info_1[f'number_of_units_p{power}'] = np.power(suburb_info_1['number_of_units'], power)

# Apply other transformations
suburb_info_1['number_of_units_log'] = np.log(suburb_info_1['number_of_units'])
suburb_info_1['number_of_units_sqrt'] = np.sqrt(suburb_info_1['number_of_units'])
suburb_info_1['number_of_units_cbrt'] = np.cbrt(suburb_info_1['number_of_units'])
suburb_info_1['number_of_units_boxcox'], _ = stats.boxcox(suburb_info_1['number_of_units'])
qt = QuantileTransformer(output_distribution='normal')
suburb_info_1['number_of_units_quantile'] = qt.fit_transform(suburb_info_1[['number_of_units']])

# Define independent and dependent variables
x = suburb_info_1[['number_of_houses', 'aus_born_perc', 'population', 'median_income']]
y = suburb_info_1['median_house_price']

# Initialize and fit model
model = LinearRegression()

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

# Initialize an empty list to store the results
results = []

# Test model for each transformation
transformations = [f'number_of_units_p{power}' for power in powers] + ['number_of_units_log', 'number_of_units_sqrt', 'number_of_units_cbrt', 'number_of_units_boxcox', 'number_of_units_quantile']
for trans in transformations:
    x_train_transformed = x_train.copy()
    x_test_transformed = x_test.copy()

    x_train_transformed['number_of_units'] = suburb_info_1.loc[x_train.index, trans]
    x_test_transformed['number_of_units'] = suburb_info_1.loc[x_test.index, trans]

    model.fit(x_train_transformed, y_train)

    y_pred = model.predict(x_test_transformed)
    r2 = r2_score(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    # Append the results to the list
    results.append([trans, r2, rmse])

# Convert the results list to a DataFrame
results_df = pd.DataFrame(results, columns=['Transformation', 'R-squared', 'Root Mean Squared Error'])

# Display the results
print(results_df)

"""<div class="alert alert-block alert-info">
    
#### **6.4.4. Transformations on aus_born_perc** <a class="anchor" name="trans_aus_born_perc"></a>

Among the transformations suitable for left-skewed data (square, cube, fourth power, inverse root, and quantile), the quantile transformation yielded the best results, with the highest R-squared value (0.668373) and the lowest RMSE (0.502465). This is because quantile transformation spreads out the data uniformly, reducing skewness effectively. By expanding smaller values more than larger ones, it normalizes the distribution of `aus_born_perc` and stabilizes variance, making the data more suitable for linear regression modeling. This transformation is especially useful when the distribution needs to be adjusted to meet the assumptions of linear regression, such as homoscedasticity and linearity.

However, the fourth power transformation also performed well, with an R-squared of 0.668313 and an RMSE of 0.502511, closely following the quantile transformation. Meanwhile, the square and cube transformations provided decent improvements in R-squared and RMSE, though not as significant as the quantile transformation. Conversely, the inverse root transformation is the least appropriate for `aus_born_perc`, though it still improved the model compared to no transformation.

In summary, quantile transformation was the most effective at addressing the left skewness of `aus_born_perc` and improving the linear regression model's overall predictive performance.
"""

# Testing on aus_born_perc:

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

powers = [-1, -0.5, 2, 3, 4]  # reciprocal, inverse root, square, cube, and fourth power

# Add a constant to the data
constant = 1 - suburb_info_1['aus_born_perc'].min()
suburb_info_1['aus_born_perc'] = suburb_info_1['aus_born_perc'] + constant

# Apply power transformations
for power in powers:
    suburb_info_1[f'aus_born_perc_p{power}'] = np.power(suburb_info_1['aus_born_perc'], power)

# Apply other transformations
suburb_info_1['aus_born_perc_log'] = np.log(suburb_info_1['aus_born_perc'])
suburb_info_1['aus_born_perc_sqrt'] = np.sqrt(suburb_info_1['aus_born_perc'])
suburb_info_1['aus_born_perc_cbrt'] = np.cbrt(suburb_info_1['aus_born_perc'])
suburb_info_1['aus_born_perc_boxcox'], _ = stats.boxcox(suburb_info_1['aus_born_perc'])
qt = QuantileTransformer(output_distribution='normal')
suburb_info_1['aus_born_perc_quantile'] = qt.fit_transform(suburb_info_1[['aus_born_perc']])

# Define independent and dependent variables
x = suburb_info_1[['number_of_houses', 'number_of_units', 'population', 'median_income']]
y = suburb_info_1['median_house_price']

# Initialize and fit model
model = LinearRegression()

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

# Initialize an empty list to store the results
results = []

# Test model for each transformation
transformations = [f'aus_born_perc_p{power}' for power in powers] + ['aus_born_perc_log', 'aus_born_perc_sqrt', 'aus_born_perc_cbrt', 'aus_born_perc_boxcox', 'aus_born_perc_quantile']
for trans in transformations:
    x_train_transformed = x_train.copy()
    x_test_transformed = x_test.copy()

    x_train_transformed['aus_born_perc'] = suburb_info_1.loc[x_train.index, trans]
    x_test_transformed['aus_born_perc'] = suburb_info_1.loc[x_test.index, trans]

    model.fit(x_train_transformed, y_train)

    y_pred = model.predict(x_test_transformed)
    r2 = r2_score(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    # Append the results to the list
    results.append([trans, r2, rmse])

# Convert the results list to a DataFrame
results_df = pd.DataFrame(results, columns=['Transformation', 'R-squared', 'Root Mean Squared Error'])

# Display the results
print(results_df)

"""<div class="alert alert-block alert-info">
    
#### **6.4.5. Transformations on population** <a class="anchor" name="trans_population"></a>

Among all the transformations appropriate for right-skewed data (log, square root, cube root, and Box-Cox), the square root transformation yielded the highest R-squared value (0.653873) and the lowest RMSE (0.513333). This indicates that the square root transformation was the most effective at addressing right skewness in the `population` variable, thus enhancing the model's accuracy and predictive performance. By normalizing the distribution and stabilizing variance, the square root transformation improved the linear relationship between the population variable and median house price, making it the most suitable transformation in this context.
"""

# Testing on population:

# Create a copy of the DataFrame
suburb_info_1 = suburb_info.copy()

powers = [-1, -0.5, 2, 3, 4]  # reciprocal, inverse root, square, cube, and fourth power

# Add a constant to the data
constant = 1 - suburb_info_1['population'].min()
suburb_info_1['population'] = suburb_info_1['population'] + constant

# Apply power transformations
for power in powers:
    suburb_info_1[f'population_p{power}'] = np.power(suburb_info_1['population'], power)

# Apply other transformations
suburb_info_1['population_log'] = np.log(suburb_info_1['population'])
suburb_info_1['population_sqrt'] = np.sqrt(suburb_info_1['population'])
suburb_info_1['population_cbrt'] = np.cbrt(suburb_info_1['population'])
suburb_info_1['population_boxcox'], _ = stats.boxcox(suburb_info_1['population'])
qt = QuantileTransformer(output_distribution='normal')
suburb_info_1['population_quantile'] = qt.fit_transform(suburb_info_1[['population']])

# Define independent and dependent variables
x = suburb_info_1[['number_of_houses', 'aus_born_perc', 'number_of_units', 'median_income']]
y = suburb_info_1['median_house_price']

# Initialize and fit model
model = LinearRegression()

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

# Initialize an empty list to store the results
results = []

# Test model for each transformation
transformations = [f'population_p{power}' for power in powers] + ['population_log', 'population_sqrt', 'population_cbrt', 'population_boxcox', 'population_quantile']
for trans in transformations:
    x_train_transformed = x_train.copy()
    x_test_transformed = x_test.copy()

    x_train_transformed['population'] = suburb_info_1.loc[x_train.index, trans]
    x_test_transformed['population'] = suburb_info_1.loc[x_test.index, trans]

    model.fit(x_train_transformed, y_train)

    y_pred = model.predict(x_test_transformed)
    r2 = r2_score(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    # Append the results to the list
    results.append([trans, r2, rmse])

# Convert the results list to a DataFrame
results_df = pd.DataFrame(results, columns=['Transformation', 'R-squared', 'Root Mean Squared Error'])

# Display the results
print(results_df)

"""<div class="alert alert-block alert-info">
    
#### **6.4.6. Linear Regression Analysis on Suburb Data (After Transformation)** <a class="anchor" name="after_trans"></a>

In this analysis, we applied appropriate transformations to the variables in our dataset to address skewness and improve the predictive performance of our linear regression model.

The transformations were chosen based on the highest R-squared value and the lowest Root Mean Squared Error (RMSE) from previous tests:
* number_of_houses: Square root transformation.
* number_of_units: Box-Cox transformation.
* population: Square root transformation.
* aus_born_perc: Quantile transformation.

After applying these transformations, we trained a linear regression model on the transformed dataset. The performance of the model was then evaluated on the test data; the R-squared value of 0.7110 indicates that approximately 71.10% of the variance in the median house price can be explained by the model using the transformed variables. This is a significant improvement compared to previous models without transformation (0.656) or with less appropriate transformations, demonstrating the effectiveness of the chosen transformations in enhancing the model's explanatory power.

Meanwhile, the RMSE value of 0.4690 shows the average deviation of the predicted median house prices from the actual prices. This lower RMSE value suggests that the model's predictions are closer to the actual values, indicating better predictive accuracy and reliability relative to the model without transformation (i.e. where RMSE was 0.511).

Therefore, we can conclude that the chosen transformations applied to the independent variables have successfully addressed skewness and stabilized variance, leading to improved model performance. The significantly higher R-squared value and lower RMSE confirm that the transformed variables have enhanced the linear regression model's ability to predict median house prices accurately.
"""

# Applying transformations to all variables (based on max R square and min RMSE)

suburb_info_2 = suburb_info.copy()


# Define the variables to be transformed
variables = ['number_of_houses', 'number_of_units', 'population', 'aus_born_perc']


# Add a constant to each variable
for var in variables:
    constant = 1 - suburb_info_2[var].min()
    suburb_info_2[var] += constant


# Apply square root transformation to 'number_of_houses'
suburb_info_2['number_of_houses'] = np.sqrt(suburb_info_2['number_of_houses'])

# Apply Box-Cox transformation to 'number_of_units'
suburb_info_2['number_of_units'], _ = boxcox(suburb_info_2['number_of_units'])

# Apply square root transformation to 'population'
suburb_info_2['population'] = np.sqrt(suburb_info_2['population'])

# Apply quantile transformation to 'aus_born_perc'
qt = QuantileTransformer(n_quantiles=min(1000, len(suburb_info_2)), output_distribution='normal', random_state=2)
suburb_info_2['aus_born_perc'] = qt.fit_transform(suburb_info_2[['aus_born_perc']])


# Define independent and dependent variables
x = suburb_info_2[['number_of_houses', 'number_of_units', 'population', 'aus_born_perc', 'median_income']]
y = suburb_info_2['median_house_price']


# Split data into training and testing sets:
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)


# Initialize and fit model:
model = LinearRegression()
model.fit(x_train, y_train)


# Make predictions and evaluate your model:
y_pred = model.predict(x_test)
r2 = r2_score(y_test, y_pred)
print(f'R-squared for the model: {r2}') # R-square value for test data
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f'RMSE: {rmse}')

"""When plotting the actual values against the predicted ones, we observe that the points in the scatter plot exhibit an approximately linear trend, indicating a relatively strong positive correlation between the actual and predicted values. This suggests that the model is performing quite well in capturing the underlying relationship in the data.

A large number of points are also close to the red line, which represents perfect correlation (where the actual values equal the predicted values). This closeness indicates good predictive accuracy for many instances in the dataset.

However, there are some points which deviate from the red line more noticeably. These deviations indicate that there is room for improvement in the model's accuracy. The discrepancies could be due to various factors such as outliers (as seen previously when plotting the distributions of our independent variables), noise in the data, or aspects of the data not well captured by the model.

Overall, the scatter plot of actual versus predicted values shows that the model is performing reasonably well, with most predictions being close to the actual values. However, the fact that the points are not super close to the red line suggests that there is still room for improvement. Further refinement of the model, possibly by exploring additional transformations, adding new features, or employing more complex modeling techniques, could enhance the model's accuracy and predictive performance.
"""

# Make predictions and evaluate your model:
y_pred = model.predict(x_test)

# Plot actual vs predicted values
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')

# Plot a line for perfect correlation
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red')

plt.show()

"""When assessing how well the linear regression model fits the transformed data, we can see that the model's performance has improved after applying the chosen transformations to our independent variables.

The R-squared value from the training data increased from 0.624 to 0.678, meaning the transformed model explained approximately 67.8% of the variance in the median house price. The adjusted R-squared value also increased to from 0.612 to 0.667, indicating a much stronger model fit. The F-statistic rose to 65.23 with a p-value of 2.21e-36, underscoring the improved statistical significance of the model.

Moreover, the transformed model's performance metrics demonstrated enhanced accuracy. The train MAE decreased from 0.3683 to 0.3376 and the train RMSE from 0.4683 to 0.4336, while the test MAE dropped from 0.3892 to 0.3513 and the test RMSE from 0.5115 to 0.4690, respectively. These lower values for both training and test datasets indicated better predictive performance and minimal overfitting compared to the initial model.

Residual analysis of the transformed model further confirmed these improvements. The residual plot showed randomly scattered residuals around zero, suggesting that the assumptions of homoscedasticity (i.e. variance of the errors is constant) and no autocorrelation were better met. The Q-Q plot of the residuals also showed most points lying on the red diagonal line, indicating that the residuals were more normally distributed, which is desirable for linear regression. Additionally, there were fewer outliers deviating from the red diagonal line compared to the initial model before any transformations, suggesting better handling of extreme values and enhanced model reliability.

Hence, these results indicate that our chosen transformations have significantly enhanced the suitability of the data for a linear regression model.
"""

# Fit the model using statsmodels to get more detailed output
x_train_sm = sm.add_constant(x_train)
model_sm = sm.OLS(y_train, x_train_sm)
results = model_sm.fit()

# Print out the statistics
print(results.summary())

# Calculate metrics
y_train_pred = results.predict(x_train_sm)
y_test_pred = results.predict(sm.add_constant(x_test))

print('Train Mean Absolute Error:', mean_absolute_error(y_train, y_train_pred))
print('Train Root Mean Squared Error:', np.sqrt(mean_squared_error(y_train, y_train_pred)))
print('Test Mean Absolute Error:', mean_absolute_error(y_test, y_test_pred))
print('Test Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_test_pred)))

# Residuals
residuals = y_train - y_train_pred

# Residual plot
sns.scatterplot(x=y_train_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.show()

# Normality of residuals
qqplot(residuals, line='s')
plt.show()

# Variance Inflation Factor
vif = pd.DataFrame()
vif["variables"] = x_train.columns
vif["VIF"] = [variance_inflation_factor(x_train.values, i) for i in range(x_train.shape[1])]
print(vif)

# Cross-validation
cross_val_scores = cross_val_score(LinearRegression(), x_train, y_train, cv=5)
print('Cross-validated scores:', cross_val_scores)

# Confidence intervals
print('Confidence intervals:')
print(results.conf_int())

"""In evaluating the adequacy of the transformations applied to our dataset, we need to consider both the improvements observed and the remaining limitations. The transformations applied â€” square root for `number_of_houses` and `population`, Box-Cox for `number_of_units`, and quantile for `aus_born_perc` â€” were selected based on their ability to enhance the model's performance metrics, specifically the R-squared and RMSE values.

**Improvements:**
1. **R-squared and Adjusted R-squared Values:**
* The R-squared value on the test data increased from 0.656 (untransformed model) to 0.711 (transformed model), indicating that the model now explains 71.1% of the variance in the median house price, up from 65.6%.
* The adjusted R-squared value also improved, which confirms that the increase in R-squared is not merely due to overfitting but reflects a genuine improvement in model performance.

2. **RMSE and MAE Values:**
* The RMSE decreased from 0.511 to 0.469, demonstrating that the model's predictions are closer to the actual values post-transformation.
* The MAE values for both training and test datasets decreased, indicating reduced average errors in predictions.

3. **Residual Analysis:**
* The residuals of the transformed model are more randomly scattered around zero, indicating that the assumptions of homoscedasticity and no autocorrelation are better met.
* The Q-Q plot shows that the residuals are more normally distributed after the transformations, reducing the impact of outliers and making the model more reliable.

**Limitations:**
1. **R-squared Value of 0.711:**
* Despite the improvements, an R-squared value of 0.711 means that 28.9% of the variance in the median house price is still unexplained by the model. While this is a reasonable result for many practical purposes, it suggests that there might be additional factors or complexities in the data that the current model does not capture.

2. **Deviations in Scatter Plot:**
* The scatter plot of actual versus predicted values shows some points deviating noticeably from the line of perfect correlation. These deviations indicate areas where the model's predictions are less accurate, highlighting potential areas for further refinement.

<br>

While the transformations applied have significantly improved the model's performance, they are not exhaustive. The accuracy of the model, as indicated by the R-squared value, suggests room for further enhancement. The current model explains a substantial portion of the variance in median house prices but still leaves a notable portion unexplained. Therefore, the transformations performed are beneficial but not necessarily sufficient. Additional data preprocessing, feature engineering, and possibly more sophisticated modeling techniques should be explored to further improve the model's accuracy and predictive power.

-------------------------------------

<div class="alert alert-block alert-warning">

## **7.  Writing Solutions to CSV File** <a class="anchor" name="write_csv"></a>

</div>

<div class="alert alert-block alert-info">
    
### **7.1. Dirty Data Solution** <a class="anchor" name="write_dirty_data"></a>
"""

# Write the DataFrame to a CSV file
final_dirty_data.to_csv('055_dirty_data_solution.csv', index=False)

# If you want to download the file to your local system from Google Colab, you can use the files module
from google.colab import files
files.download('055_dirty_data_solution.csv')

"""<div class="alert alert-block alert-info">
    
### **7.2. Outlier Data Solution** <a class="anchor" name="write_outlier_data"></a>
"""

# Write the DataFrame to a CSV file
final_outlier_df.to_csv('055_outlier_data_solution.csv', index=False)

# If you want to download the file to your local system from Google Colab, you can use the files module
from google.colab import files
files.download('055_outlier_data_solution.csv')

"""<div class="alert alert-block alert-info">
    
### **7.3. Missing Data Solution** <a class="anchor" name="write_missing_data"></a>
"""

# Write the DataFrame to a CSV file
missing_data.to_csv('055_missing_data_solution.csv', float_format='%.4f', index=False)

# If you want to download the file to your local system from Google Colab, you can use the files module
from google.colab import files
files.download('055_missing_data_solution.csv')

"""-------------------------------------

<div class="alert alert-block alert-warning">

## **8. Summary** <a class="anchor" name="summary"></a>

</div>

<u>**Task 1:**</u>

Our approach to data cleaning begins with addressing missing data in `missing_data.csv`. Since this dataset contains no anomalies other than missing values, we can use a `Linear Regression` model to predict and fill in the missing values with high accuracy. This model has proven to be very useful in later stages of our data cleansing process.

Next, we address data anomalies in `dirty_data.csv`:

* `Date`:
  
  The `date` column contains dates in various formats. We used `regular expressions` to filter out incorrectly formatted dates and convert them to the correct format.

* `Customer_lat` & `Customer_lon`:
  
  We corrected the `customer_lat` and `customer_lon` columns by verifying their coordinates against the given nodes in `node.csv`, ensuring each customer belongs to a valid node.

* `Order_type`:

  We identified anomalies in the `order_type` column by checking whether the order type (`Breakfast`, `Lunch`, `Dinner`) was ordered at the appropriate time.

* `Order_price`:

  We detected and corrected anomalies by first determining the unit price for each item using the Python package `numpy.linalg.solve`. With the unit price for each item, we easily corrected the `order_price`.

* `Order_items`:
  
  We detected anomalies by categorizing which items belong to `Breakfast`, `Lunch`, or `Dinner`, then checked if these items were ordered at the correct times.

* `Distance_to_customer_KM` & `Branch_code`:

  To calculate the shortest distance from the branch to the customer, we first formed a graph using data from `nodes.csv`, `edges.csv`, and `branches.csv`, then applied `Dijkstra's algorithm`. We used these distances to detect and correct any anomalies in the `branch_code` and `distance_to_customer_KM` columns.

* `CustomerHasLoyalty?`:

  To detect and correct anomalies in the `CustomerHasLoyalty?` column, we used our trained linear model to predict the `delivery_fee`. By comparing the predicted and actual `delivery_fee`, we identified and corrected incorrect loyalty status.

After cleaning the dirty data, we proceeded to detect and remove all outliers with respect to the `delivery_fee` column in `outliers.csv`. Since the `delivery_fee` is a multivariate outlier, we used our trained model to predict the `delivery_fee`, calculated the `residuals`, and then used the `Interquartile Range (IQR)` method to detect and remove outliers.

Overall, by systematically addressing missing data, correcting anomalies, and removing outliers, we have significantly improved the quality and accuracy of our dataset. These rigorous data cleaning steps ensure that our models can make more accurate predictions and provide more reliable insights.

<u>**Task 2**</u>

Our data reshaping analysis began with identifying the robust scaling method as the optimal choice for handling outliers in our suburb data, particularly in variables such as `number_of_houses` and `number_of_units`. Robust scaling, with its ability to normalize data based on percentiles, effectively minimized the influence of extreme values while maintaining data integrity and facilitating better analysis.

This approach was followed by an initial assessment of predictive power using a linear regression model on unscaled data, serving as a baseline for future analyses. Subsequently, various transformations were applied to address skewness and enhance model performance. These transformations, tailored to each variable, resulted in a notable improvement in the model's explanatory power and predictive accuracy. Through residual analysis and comparison of performance metrics, we confirmed the effectiveness of the chosen transformations (i.e. square root for `number_of_houses` and `population`, Box-Cox for `number_of_units` and quantile for `aus_born_perc`) in enhancing the suitability of our data for linear regression modeling.

Overall, our journey underscored the importance of thoughtful data preprocessing and transformation in maximizing the predictive capabilities of our models, ultimately leading to more reliable and insightful analyses of suburb data.

-------------------------------------

<div class="alert alert-block alert-warning">

## **9. References** <a class="anchor" name="Ref"></a>

</div>

[1] numpy.linalg.solve. https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve


[2] sklearn.linear_model.LinearRegression. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html

[3] Transforming Skewed Data: How to choose the right transformation for your distribution. https://anatomisebiostats.com/biostatistics-blog/transforming-skewed-data/

[4] Transformations in Regression. https://stattrek.com/regression/linear-transformation

[5] Data Analysis Toolkit #3: Tools for Transforming Data. https://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_03.pdf

## --------------------------------------------------------------------------------------------------------------------------
"""